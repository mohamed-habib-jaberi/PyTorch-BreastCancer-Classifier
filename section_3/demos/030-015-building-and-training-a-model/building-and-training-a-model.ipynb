{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Neural Network\n",
    "In this demo we are going to demonstrate how to build and train a model using PyTorch.\n",
    "\n",
    "This model will be a neural network type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Class\n",
    "REVIEW: Building a neural network is made simple in PyTorch.\n",
    "\n",
    "This is because of the `nn.Module` which we inherit when we create our class to simplify building, managing and organizing our model.\n",
    "\n",
    "This is used to lay the blueprint for our model.\n",
    "\n",
    "### Structure of our Class\n",
    "\n",
    "`__init__()`: This is where we define the layers of our network.\n",
    "\n",
    "\n",
    "`forward()`: This is where we define how data is processed through our layers to get a prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the nn module\n",
    "import torch.nn as nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple class\n",
    "class SimpleNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNeuralNetwork, self).__init__() # initialize superclass for automatic parameters\n",
    "        \n",
    "        # Define the layers: an input layer, a hidden layer, and an output layer\n",
    "        self.input_layer = nn.Linear(10, 20)  # Input size of 10, output size of 20\n",
    "        self.hidden_layer = nn.Linear(20, 15) # Hidden layer with input size of 20, output size of 15\n",
    "        self.output_layer = nn.Linear(15, 1)  # Output layer with input size of 15, output size of 1\n",
    "        \n",
    "        # Define the activation function (introduces non-linearity into the model)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    # Define the forward pass\n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.input_layer(x))  # Pass data through the input layer\n",
    "        x = self.activation(self.hidden_layer(x)) # Pass data through the hidden layer\n",
    "        x = self.output_layer(x)                  # Pass data through the output layer (no activation here)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10])\n"
     ]
    }
   ],
   "source": [
    "# Demonstrate our model with an example\n",
    "import torch\n",
    "\n",
    "# Create a tensor with shape (5, 10) - batch of 5 samples, each with 10 features\n",
    "example_tensor = torch.randn(5, 10)\n",
    "print(example_tensor.size())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 20])\n"
     ]
    }
   ],
   "source": [
    "# Create the input layer as it is in our Class\n",
    "input_layer = nn.Linear(10, 20)\n",
    "\n",
    "# Run the example through our input layer\n",
    "input_linear_example = input_layer(example_tensor)\n",
    "print(input_linear_example.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 15])\n"
     ]
    }
   ],
   "source": [
    "# Do the same but with the hidden layer\n",
    "hidden_layer = nn.Linear(20, 15)\n",
    "\n",
    "# Run the input_linear_example through hidden layer\n",
    "hidden_linear_example = hidden_layer(input_linear_example)\n",
    "print(hidden_linear_example.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1])\n"
     ]
    }
   ],
   "source": [
    "# Same for the output layer\n",
    "output_layer = nn.Linear(15, 1)\n",
    "\n",
    "# Run hidden_linear_example through output layer\n",
    "ouput_linear_example = output_layer(hidden_linear_example)\n",
    "print(ouput_linear_example.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before ReLU: tensor([[-0.2273],\n",
      "        [-0.1336],\n",
      "        [-0.1640],\n",
      "        [-0.3476],\n",
      "        [ 0.0086]], grad_fn=<AddmmBackward0>)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Now with activation layer ReLU befor and after on the output example\n",
    "print(f\"Before ReLU: {ouput_linear_example}\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After ReLU: tensor([[0.0000],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.0000],\n",
      "        [0.0086]], grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Run through ReLU\n",
    "activation_relu_example = nn.ReLU()(ouput_linear_example)\n",
    "print(f\"After ReLU: {activation_relu_example}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Tensor:\n",
      "tensor([[ 6.4633e-01,  1.4995e+00, -4.7450e-04, -2.1584e+00,  1.6394e+00,\n",
      "          7.2253e-01,  1.1799e-01, -2.1203e-01, -1.6965e-01,  4.4847e-01],\n",
      "        [ 3.6641e-01, -5.8538e-02,  1.2369e+00,  3.8564e-01,  4.8086e-01,\n",
      "          4.0321e-01,  1.9775e-01,  1.9555e-01,  1.0579e+00,  1.2314e+00],\n",
      "        [ 8.3325e-02, -9.7946e-02,  1.0330e-02, -1.4780e-01, -2.1924e-01,\n",
      "         -1.4236e-01, -6.5612e-01,  1.7550e+00, -4.6690e-01,  6.7153e-01],\n",
      "        [ 8.8750e-01,  9.9059e-01,  4.5990e-01, -6.6787e-01, -9.3856e-02,\n",
      "         -6.3551e-01, -1.7154e+00, -6.2987e-01,  1.7602e+00,  9.7986e-01],\n",
      "        [-3.7544e-01, -1.4572e+00,  1.3938e+00, -8.7970e-01, -1.8274e+00,\n",
      "          3.0554e-01, -1.1361e-01,  1.5224e+00, -5.8151e-01, -1.8045e-01]])\n",
      "\n",
      "Output Tensor:\n",
      "tensor([[-0.2184],\n",
      "        [-0.1969],\n",
      "        [-0.1859],\n",
      "        [-0.2171],\n",
      "        [-0.1646]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Create an example with forward\n",
    "\n",
    "# Recreate instance of activation layer\n",
    "activation = nn.ReLU()\n",
    "\n",
    "# Pass example through input layer and apply ReLU\n",
    "x = activation(input_layer(example_tensor))\n",
    "# Pass through hidden layer and apply ReLU \n",
    "x = activation(hidden_layer(x))\n",
    "# Pass through output layer (no activation)\n",
    "output = output_layer(x)\n",
    "\n",
    "print(\"Example Tensor:\")\n",
    "print(example_tensor)\n",
    "print(\"\\nOutput Tensor:\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = SimpleNeuralNetwork()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNeuralNetwork(\n",
      "  (input_layer): Linear(in_features=10, out_features=20, bias=True)\n",
      "  (hidden_layer): Linear(in_features=20, out_features=15, bias=True)\n",
      "  (output_layer): Linear(in_features=15, out_features=1, bias=True)\n",
      "  (activation): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Show the layers\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Parameters\n",
    "Layers have associate weights and biases.\n",
    "\n",
    "These weights and biases get adjusted during model training.\n",
    "\n",
    "Lucky for us, the adjustments are tracked automatically by PyTorch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: input_layer.weight | Size: torch.Size([20, 10]) | Values : tensor([[ 0.0392,  0.1515, -0.2488, -0.0223,  0.1219,  0.2216,  0.1022, -0.1129,\n",
      "          0.2168, -0.1572],\n",
      "        [-0.2833, -0.2092, -0.1960,  0.1560,  0.2170, -0.3085, -0.0034,  0.1474,\n",
      "          0.0305,  0.2033]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: input_layer.bias | Size: torch.Size([20]) | Values : tensor([-0.2154, -0.1001], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: hidden_layer.weight | Size: torch.Size([15, 20]) | Values : tensor([[-5.2847e-02,  2.0027e-04,  1.5755e-01,  1.4706e-01,  1.6625e-01,\n",
      "          1.0633e-01, -1.3583e-01, -5.3231e-02, -8.4812e-02, -3.3965e-02,\n",
      "          8.1506e-02,  6.7148e-02,  6.9493e-02, -1.6088e-01, -1.3256e-01,\n",
      "         -3.9679e-02, -1.8114e-01,  3.3114e-02,  1.0906e-01,  3.9972e-02],\n",
      "        [ 1.7438e-02,  2.0866e-01,  9.6094e-02,  1.7150e-01,  4.7751e-02,\n",
      "         -4.6895e-02,  6.5745e-02, -1.3358e-01, -1.2218e-01,  4.5894e-02,\n",
      "         -1.9258e-01,  1.5990e-01,  1.6079e-01,  7.7859e-02, -7.8815e-02,\n",
      "         -1.8919e-01, -1.6154e-01,  2.8002e-02,  3.0504e-02,  1.8090e-01]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: hidden_layer.bias | Size: torch.Size([15]) | Values : tensor([-0.1305, -0.0367], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: output_layer.weight | Size: torch.Size([1, 15]) | Values : tensor([[-0.0844,  0.0252, -0.2039,  0.1243, -0.2196,  0.1042,  0.0131,  0.2415,\n",
      "          0.0855,  0.0556,  0.0729, -0.1909,  0.2127,  0.0528,  0.0465]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: output_layer.bias | Size: torch.Size([1]) | Values : tensor([0.1098], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Loop through the parameters in human readable\n",
    "for name, param in model.named_parameters():\n",
    "    print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:2]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we see the current shape and values for each layers weight and bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 3.9218e-02,  1.5150e-01, -2.4881e-01, -2.2315e-02,  1.2193e-01,\n",
      "          2.2158e-01,  1.0221e-01, -1.1285e-01,  2.1681e-01, -1.5720e-01],\n",
      "        [-2.8330e-01, -2.0917e-01, -1.9603e-01,  1.5597e-01,  2.1704e-01,\n",
      "         -3.0851e-01, -3.3981e-03,  1.4735e-01,  3.0495e-02,  2.0333e-01],\n",
      "        [ 2.7765e-01,  6.6041e-02,  7.0729e-02,  1.6976e-01,  2.2319e-01,\n",
      "          1.0814e-01,  2.4366e-02, -2.5108e-01, -1.0507e-01, -1.7668e-01],\n",
      "        [ 2.6568e-01, -3.9034e-03,  2.6493e-01,  1.7411e-01,  2.7335e-01,\n",
      "         -1.8790e-01, -2.4024e-01, -1.5408e-01, -1.5831e-02, -2.2285e-01],\n",
      "        [-3.0120e-01, -2.1881e-01,  9.2978e-02,  2.8449e-01, -4.1395e-02,\n",
      "         -2.0326e-01,  2.3159e-01, -5.9635e-02,  7.7009e-02, -2.8341e-02],\n",
      "        [ 8.4580e-02, -2.7772e-01, -7.8919e-02, -1.9953e-01, -7.7976e-02,\n",
      "         -1.5422e-01, -3.0460e-01, -1.1211e-02,  8.8838e-02,  1.2237e-01],\n",
      "        [-1.5768e-01, -2.1675e-01, -2.2207e-02, -2.0532e-01, -2.0846e-01,\n",
      "         -3.7782e-02, -1.7719e-01,  9.3477e-02,  8.9519e-02,  2.9639e-02],\n",
      "        [ 3.7014e-02,  5.0636e-02,  1.2624e-01, -1.2594e-02,  2.5626e-01,\n",
      "          9.1556e-02,  2.0902e-01, -2.1634e-01,  5.0018e-02, -2.5590e-01],\n",
      "        [-1.9179e-01, -2.9494e-01,  1.9974e-01,  1.4161e-01, -1.7668e-01,\n",
      "          1.6393e-01, -2.1172e-01, -2.2029e-02,  1.5398e-02, -1.2665e-01],\n",
      "        [-1.3193e-01,  1.4767e-01, -2.0957e-01,  2.6975e-01, -2.4453e-01,\n",
      "          2.7352e-01, -1.4815e-04,  2.2370e-01,  3.3524e-03, -6.6835e-02],\n",
      "        [-1.5664e-01, -3.1536e-01, -2.8455e-01,  1.8896e-01, -2.6305e-01,\n",
      "         -2.6789e-01, -3.1222e-02, -3.0032e-01, -2.0814e-01,  1.9659e-01],\n",
      "        [ 2.1672e-01,  2.4178e-01,  2.0088e-01, -2.1167e-01,  2.4207e-01,\n",
      "         -1.1528e-01,  7.3739e-03, -2.6703e-01,  1.6932e-01,  2.9581e-01],\n",
      "        [-5.7007e-03,  1.7430e-01,  5.5812e-02,  6.6168e-02,  3.4314e-02,\n",
      "          1.5932e-01, -4.9126e-02,  1.6364e-01, -1.8548e-01,  2.0100e-01],\n",
      "        [-2.3455e-01,  2.5811e-01, -1.5693e-01, -2.2147e-01,  2.2613e-01,\n",
      "          2.7166e-01,  7.2129e-02, -1.2087e-01,  2.7126e-01, -1.7422e-01],\n",
      "        [ 1.0041e-01, -1.9666e-01,  1.7164e-01, -2.3438e-01, -1.0464e-01,\n",
      "         -1.7086e-01,  2.1541e-01, -2.4897e-01,  5.2810e-02,  2.3233e-01],\n",
      "        [-6.4479e-02, -3.1518e-01,  1.3123e-01,  2.9084e-02, -1.5124e-01,\n",
      "         -4.8200e-02, -2.1560e-01,  2.6023e-01,  1.2847e-01,  1.7607e-02],\n",
      "        [-1.8477e-01,  2.0004e-01,  2.0163e-01,  2.9574e-01,  1.7052e-01,\n",
      "          1.8397e-01,  1.1481e-01,  2.2505e-02, -1.1535e-01, -1.8825e-01],\n",
      "        [ 2.5982e-01, -2.6308e-01,  1.5183e-01, -1.8453e-01,  3.0719e-01,\n",
      "          2.9170e-01,  8.7462e-02,  3.1582e-01,  2.8753e-01, -1.3340e-01],\n",
      "        [ 1.7443e-01,  3.5064e-02, -1.1556e-02, -1.0372e-01, -2.5865e-01,\n",
      "         -1.8286e-01, -2.1208e-01,  2.6248e-02, -9.6508e-02,  8.5167e-02],\n",
      "        [ 8.6282e-03, -1.2979e-01, -5.3382e-02,  2.6736e-02, -2.6799e-01,\n",
      "         -1.3008e-01,  1.6786e-01, -2.0903e-01,  6.6736e-02,  1.4631e-01]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2154, -0.1001,  0.2534, -0.1083,  0.0444, -0.1108,  0.1655,  0.1197,\n",
      "         0.1861,  0.2207,  0.0899, -0.1110,  0.0813,  0.2644, -0.2203, -0.0594,\n",
      "         0.0277,  0.0365,  0.2315, -0.2803], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-5.2847e-02,  2.0027e-04,  1.5755e-01,  1.4706e-01,  1.6625e-01,\n",
      "          1.0633e-01, -1.3583e-01, -5.3231e-02, -8.4812e-02, -3.3965e-02,\n",
      "          8.1506e-02,  6.7148e-02,  6.9493e-02, -1.6088e-01, -1.3256e-01,\n",
      "         -3.9679e-02, -1.8114e-01,  3.3114e-02,  1.0906e-01,  3.9972e-02],\n",
      "        [ 1.7438e-02,  2.0866e-01,  9.6094e-02,  1.7150e-01,  4.7751e-02,\n",
      "         -4.6895e-02,  6.5745e-02, -1.3358e-01, -1.2218e-01,  4.5894e-02,\n",
      "         -1.9258e-01,  1.5990e-01,  1.6079e-01,  7.7859e-02, -7.8815e-02,\n",
      "         -1.8919e-01, -1.6154e-01,  2.8002e-02,  3.0504e-02,  1.8090e-01],\n",
      "        [ 8.2975e-02,  1.0149e-01,  1.1367e-01, -2.0684e-01,  1.4983e-01,\n",
      "          1.5000e-03, -1.8490e-01, -1.7477e-01,  1.5016e-01, -7.8458e-03,\n",
      "         -1.5589e-01, -7.2154e-02, -1.1193e-01,  7.5258e-02, -9.9523e-02,\n",
      "          1.5114e-01,  1.2840e-02, -1.5765e-01,  1.1271e-02,  1.4136e-01],\n",
      "        [ 1.9092e-01, -1.8304e-02,  1.4315e-01, -2.6119e-02,  3.7431e-02,\n",
      "         -1.9726e-01,  2.1746e-01, -4.5210e-02, -1.4205e-01, -3.5837e-02,\n",
      "          1.4320e-01,  1.1342e-01,  1.0160e-01,  1.2750e-01,  1.3428e-01,\n",
      "         -9.7725e-02,  1.6850e-01, -1.7166e-01, -6.7165e-02,  7.7155e-02],\n",
      "        [ 4.5787e-02, -4.3760e-02, -1.0177e-01, -5.0024e-02, -3.2188e-02,\n",
      "          1.5862e-01,  3.0562e-02, -1.3148e-01, -1.3009e-01, -2.1755e-01,\n",
      "         -1.7456e-01,  2.0359e-01,  2.2124e-01,  9.8029e-02, -1.9045e-01,\n",
      "          1.1605e-01,  2.1230e-01,  1.4857e-01,  1.3170e-02,  3.8999e-02],\n",
      "        [-8.8081e-02,  4.4185e-02,  1.2377e-01, -1.1989e-02,  1.1590e-01,\n",
      "         -2.0862e-01, -1.5040e-01,  3.9429e-02,  9.1649e-02,  9.6235e-02,\n",
      "          1.9987e-01,  1.2543e-02,  1.8584e-01,  2.9392e-02, -1.9179e-01,\n",
      "          1.3331e-01, -1.6373e-01,  1.0518e-01, -1.3936e-01, -5.2619e-02],\n",
      "        [-2.0408e-01, -1.8033e-01,  5.9507e-02,  1.6880e-01,  4.7887e-03,\n",
      "         -8.7735e-02, -1.5860e-01,  1.7535e-01,  9.0649e-02,  2.2098e-01,\n",
      "          7.0580e-03,  1.3328e-01, -3.8009e-04, -4.9119e-02, -5.6579e-02,\n",
      "         -1.1985e-01,  1.4472e-01, -5.9875e-02, -1.0437e-01, -3.3543e-02],\n",
      "        [-3.4533e-02, -1.3778e-01,  9.7612e-02, -1.4121e-01,  1.9807e-01,\n",
      "         -2.2210e-01,  3.0939e-03,  1.0285e-01,  1.9755e-01, -1.8616e-01,\n",
      "          7.7402e-02,  6.5226e-02, -1.3903e-01, -1.9528e-01,  8.0994e-02,\n",
      "          2.8585e-02,  8.3576e-02, -3.0283e-02, -4.6480e-02,  2.0689e-01],\n",
      "        [ 1.0160e-01,  1.6424e-01, -8.6534e-02,  1.8315e-01,  2.1434e-01,\n",
      "          7.4899e-02, -2.5235e-02,  3.0127e-02,  2.2286e-01,  5.6730e-02,\n",
      "          1.3735e-01,  1.4432e-01, -2.2088e-01,  9.3207e-02,  5.9668e-02,\n",
      "         -3.0556e-02,  9.3207e-02,  1.0891e-02,  1.9597e-01,  2.0818e-01],\n",
      "        [-9.5787e-02,  1.3125e-01,  2.1441e-01,  8.7075e-03, -1.1566e-01,\n",
      "          1.5482e-01, -4.0165e-02, -1.8496e-02, -1.3230e-01,  4.6667e-02,\n",
      "          8.3679e-02, -1.8357e-01, -1.5510e-01, -1.7539e-01,  1.1477e-01,\n",
      "         -1.9765e-01,  5.6679e-02,  2.0864e-01,  5.2146e-02,  6.0412e-02],\n",
      "        [ 2.0515e-01, -2.2079e-01, -1.3625e-01, -1.3718e-01,  3.7269e-03,\n",
      "          4.6078e-02, -1.4170e-01, -1.8115e-01,  6.2640e-02, -9.1646e-02,\n",
      "         -2.8048e-02,  8.6203e-03,  1.4529e-02, -7.4780e-02,  1.3478e-01,\n",
      "          1.9336e-03,  2.3466e-02, -1.3114e-01, -5.9096e-02, -1.9252e-01],\n",
      "        [ 1.5226e-01, -5.8758e-03,  8.8356e-02, -1.5481e-01, -1.2827e-01,\n",
      "          3.5631e-02,  2.0561e-01,  6.9103e-02,  1.3156e-01,  1.1851e-01,\n",
      "          1.4633e-01,  1.5165e-01, -3.6835e-02,  1.9239e-01, -1.3190e-01,\n",
      "         -1.7914e-01, -1.9593e-01,  1.5961e-01, -2.1652e-01,  2.1980e-01],\n",
      "        [-6.6998e-02,  1.0978e-01,  1.4665e-01,  1.9282e-02,  3.0562e-02,\n",
      "         -1.5269e-01, -1.2230e-01, -1.3092e-01,  1.9261e-01,  6.9062e-02,\n",
      "         -1.7978e-02, -2.1524e-02, -5.8627e-02,  1.3493e-01,  1.5921e-01,\n",
      "         -4.7134e-02,  8.2489e-02,  6.5141e-02, -6.4194e-02,  1.0583e-02],\n",
      "        [ 1.5594e-01, -1.3884e-01,  4.1794e-03,  6.6765e-02,  1.6529e-01,\n",
      "          9.5313e-02, -1.1901e-02,  2.1192e-02,  2.5551e-02, -1.1077e-01,\n",
      "         -1.7073e-01,  3.2265e-02,  1.1050e-01,  1.0316e-01, -1.0298e-01,\n",
      "          1.6564e-01,  1.4676e-01,  6.4228e-02, -1.8533e-02,  1.1867e-01],\n",
      "        [ 7.5148e-02,  1.1071e-01,  1.1983e-01,  1.6691e-01,  3.0070e-02,\n",
      "          1.7329e-01, -1.0901e-01,  1.8660e-01, -4.1355e-02,  1.0052e-01,\n",
      "         -4.1178e-02, -4.7381e-02,  3.5466e-02,  4.7944e-02,  1.9630e-01,\n",
      "         -1.9390e-01, -1.1920e-01,  1.3633e-01, -1.5059e-01, -1.8349e-01]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.1305, -0.0367, -0.1259,  0.1358,  0.1917, -0.0244, -0.1549,  0.1361,\n",
      "         0.0582,  0.0841,  0.1270,  0.0403,  0.0632,  0.1119,  0.1604],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([[-0.0844,  0.0252, -0.2039,  0.1243, -0.2196,  0.1042,  0.0131,  0.2415,\n",
      "          0.0855,  0.0556,  0.0729, -0.1909,  0.2127,  0.0528,  0.0465]],\n",
      "       requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.1098], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Another way to display parameters\n",
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review Autograd\n",
    "In PyTorch, autograd automatically computes gradients, which is essential for training a neural network by adjusting its weights to improve predictions.\n",
    "\n",
    "`model.parameters()` provides access to the model’s weights and biases, which are PyTorch tensors that have `requires_grad=True`. \n",
    "\n",
    "This means they automatically participate in PyTorch's autograd system, which tracks operations on these tensors to build a computation graph.\n",
    "\n",
    "Very Powerful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define a Loss Function and Optimizer\n",
    "\n",
    "REVIEW:\n",
    "\n",
    "Loss Function: Measures how well the model's predictions match the actual data, guiding the model on how much to adjust to improve.\n",
    "\n",
    "\n",
    "Optimizer: Updates the model's parameters based on the loss, using methods like gradient descent to minimize errors and improve performance over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a common loss function for an Image Classifier\n",
    "\n",
    "# Part of the nn module\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the loss function\n",
    "criterion = nn.CrossEntropyLoss() # commonly used for classification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import optimizer modules\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an optimizer instance and provide it the parameters\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9) # SGD is commonly used in classifcation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Go through the steps of Training a Model\n",
    "\n",
    "But first we must create our data and transformations.\n",
    "\n",
    "We are going to use the MNIST preloaded Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26.4M/26.4M [00:00<00:00, 27.9MB/s]\n",
      "100%|██████████| 29.5k/29.5k [00:00<00:00, 1.64MB/s]\n",
      "100%|██████████| 4.42M/4.42M [00:00<00:00, 24.0MB/s]\n",
      "100%|██████████| 5.15k/5.15k [00:00<00:00, 29.5MB/s]\n"
     ]
    }
   ],
   "source": [
    "import torchvision.datasets\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "# Define Transforms. Already resized.\n",
    "transform = v2.Compose(\n",
    "    [v2.ToImage(), \n",
    "     v2.ToDtype(torch.float32, scale=True),\n",
    "     v2.Normalize((0.5,), (0.5,))]) # These are grayscale images\n",
    "\n",
    "# Training dataset and dataloader\n",
    "train_dataset = torchvision.datasets.FashionMNIST(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32,\n",
    "                                          shuffle=True, num_workers=1)\n",
    "\n",
    "# Validation dataset and dataloader\n",
    "val_dataset = torchvision.datasets.FashionMNIST(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32,\n",
    "                                         shuffle=False, num_workers=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a NN for an Image Classifier\n",
    "Here we are going to create a Neural Network to train an image classifcation model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Class\n",
    "import torch.nn as nn\n",
    "# This module simplifies a way to import Operations (Activation Functions)\n",
    "import torch.nn.functional as F \n",
    "\n",
    "\n",
    "class ImageClassificationNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageClassificationNet, self).__init__()\n",
    "        # Takes an input with 1 channel , outputs 6 feature maps, uses a 5x5 kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        # Takes 6 input feature maps from the previous layer, outputs 16 feature maps, uses a 5x5 kernel\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # Define a max pooling layer to downsample the feature maps by a factor of 2\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        # Takes the flattened output from the convolutional layers (16 feature maps of size 5x5) and outputs 120 units\n",
    "        self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
    "        # Define the second fully connected layer, which maps 120 units to 84 units\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        # 10 classes for classification\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input `x` through the first convolutional layer, apply ReLU activation, then apply max pooling\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # Pass the result through the second convolutional layer, apply ReLU activation, then apply max pooling\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # Flatten the feature maps into a 1D vector, keeping the batch dimension\n",
    "        x = torch.flatten(x, 1)\n",
    "        # Pass through the first fully connected layer and apply ReLU activation\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # Pass through the second fully connected layer and apply ReLU activation\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # Pass through the third fully connected layer to get the output (raw scores for each class)\n",
    "        x = self.fc3(x)\n",
    "        # Return the output scores (logits) for each class\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember how to check for GPU?\n",
    "import torch\n",
    "\n",
    "# Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ImageClassificationNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=256, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Creates an instance of our model\n",
    "model = ImageClassificationNet().to(device)\n",
    "\n",
    "# Print it\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Funtion and Optimizer\n",
    "Use same as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Training Loop\n",
    "A training loop in PyTorch is the process of iteratively feeding data through a model, calculating the loss, and updating the model’s parameters to minimize that loss. \n",
    "\n",
    "This loop continues for a set number of epochs or until the model reaches satisfactory performance on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our number of training loops\n",
    "N_EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 1.4718006820042928\n",
      "Epoch: 1 Loss: 0.5883237002849578\n",
      "Epoch: 2 Loss: 0.4931527696251869\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):  # Loop over the dataset N_EPOCH times\n",
    "    \n",
    "    running_loss = 0.0  # Initialize the running loss for the current epoch\n",
    "    \n",
    "    # Loop over the training data in batches\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data  # Unpack the data; inputs are the images, labels are the classes\n",
    "        \n",
    "        optimizer.zero_grad()  # Clear the gradients for the optimizer to avoid accumulation from previous steps\n",
    "\n",
    "        outputs = model(inputs)  # Forward pass: compute the model's predictions on the inputs\n",
    "        loss = criterion(outputs, labels)  # Calculate the loss by comparing predictions to true labels\n",
    "        loss.backward()  # Backward pass: compute gradients of the loss with respect to model parameters\n",
    "        optimizer.step()  # Update model parameters based on the computed gradients\n",
    "        \n",
    "        running_loss += loss.item()  # Accumulate the loss for the current epoch\n",
    "\n",
    "    # Print the average loss for this epoch by dividing the accumulated loss by the number of batches\n",
    "    print(f\"Epoch: {epoch} Loss: {running_loss/len(train_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a Training Loop with Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Train Loss: 0.43733544745047886 Val Loss: 0.4498921820340446\n",
      "Epoch: 1 Train Loss: 0.4028322628815969 Val Loss: 0.4282173962591174\n",
      "Epoch: 2 Train Loss: 0.377310985426108 Val Loss: 0.3835013898940513\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(N_EPOCHS):  # Loop over the dataset N_EPOCH times\n",
    "    \n",
    "    ####### TRAINING\n",
    "    training_loss = 0.0  # Initialize the training loss for the current epoch\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    # Loop over the training data in batches\n",
    "    for i, data in enumerate(train_loader, 0):\n",
    "        inputs, labels = data  # Unpack the data; inputs are the images, labels are the classes\n",
    "        \n",
    "        optimizer.zero_grad()  # Clear the gradients for the optimizer to avoid accumulation from previous steps\n",
    "\n",
    "        outputs = model(inputs)  # Forward pass: compute the model's predictions on the inputs\n",
    "        loss = criterion(outputs, labels)  # Calculate the loss by comparing predictions to true labels\n",
    "        loss.backward()  # Backward pass: compute gradients of the loss with respect to model parameters\n",
    "        optimizer.step()  # Update model parameters based on the computed gradients\n",
    "        \n",
    "        training_loss += loss.item()  # Accumulate the training loss for the current epoch\n",
    "\n",
    "    ######## VALIDATION\n",
    "    val_loss = 0.0 # Initialize the validation loss for the current epoch\n",
    "    # Set the model to evaluation \n",
    "    model.eval()\n",
    "\n",
    "    # Loop over the validation data in batches\n",
    "    for i, data in enumerate(val_loader, 0):\n",
    "        inputs, labels = data  # Unpack the data like we do above\n",
    "        \n",
    "        outputs = model(inputs)  # Compute predictions\n",
    "        loss = criterion(outputs, labels)  # Calculate the loss by\n",
    "        \n",
    "        #### NOTICE we do not compute gradients and/or adjust weights #### \n",
    "        val_loss += loss.item()  # Accumulate the loss for the current epoch\n",
    "\n",
    "    # Print the training loss and the val loss\n",
    "    print(f\"Epoch: {epoch} Train Loss: {training_loss/len(train_loader)} Val Loss: {val_loss/len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# About Loss\n",
    "If validation continues to decrease, its performing well.\n",
    "\n",
    "If training continues to decrease but validation does not, its likely that its overfitting.\n",
    "\n",
    "Likely we would need many more epochs to train an accurate model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
