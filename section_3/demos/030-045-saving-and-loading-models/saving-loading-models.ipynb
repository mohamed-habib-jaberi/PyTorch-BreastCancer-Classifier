{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Loading Models\n",
    "PyTorch provides several methods for saving and loading models.\n",
    "\n",
    "This Demo will cover several methods using an example Model. \n",
    "\n",
    "#### Functions for Saving and Loading \n",
    "`torch.save()`: Save PyTorch objects (models, tensors, dictionaries, etc...) using Pythons pickle module.\n",
    "\n",
    "`torch.load()`: Loads PyTorch objects into memory.\n",
    "\n",
    "`load_state_dict()`: Loads saved parameters from objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Fake Model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FakeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FakeNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 50)\n",
    "        self.batch_norm = nn.BatchNorm1d(50) \n",
    "        self.fc2 = nn.Linear(50, 1)        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))              \n",
    "        x = self.batch_norm(x)               \n",
    "        x = self.fc2(x)                      \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FakeNet(\n",
      "  (fc1): Linear(in_features=10, out_features=50, bias=True)\n",
      "  (batch_norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=50, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create our model\n",
    "model = FakeNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fake dataset\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class FakeDataset(Dataset):\n",
    "    def __init__(self, num_samples=1000):\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Generate random input data with 10 features\n",
    "        x = torch.randn(10)\n",
    "        # Generate a random target value\n",
    "        y = torch.randn(1)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "\n",
    "# Create a dataset and data loader\n",
    "dataset = FakeDataset(num_samples=1000)\n",
    "data_loader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create loss and optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a fake model\n",
    "N_EPOCHS = 5\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, targets) in enumerate(data_loader):\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss\n",
    "        running_loss += loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Loading using `state_dict`\n",
    "`state_dict` is a dictionary that stores all the learnable parameters of a model, like weights and biases as well as hyperparameters of an Optimizer. This makes it easy to save, load, and transfer the model’s parameters, allowing flexible model saving and reloading across different environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({'fc1.weight': tensor([[-0.2281, -0.0470,  0.2462, -0.2436, -0.0946, -0.3025,  0.1570,  0.0669,\n",
      "          0.1495,  0.3131],\n",
      "        [ 0.1679, -0.3009,  0.3013,  0.1753,  0.0325, -0.1105, -0.2211, -0.2457,\n",
      "         -0.1122,  0.2106],\n",
      "        [-0.1523, -0.2123, -0.1113, -0.2280, -0.3060, -0.0035, -0.0418,  0.1317,\n",
      "         -0.2353, -0.0075],\n",
      "        [ 0.0966, -0.2417, -0.1267, -0.1652,  0.3085, -0.2116,  0.2211,  0.0841,\n",
      "         -0.1746, -0.1794],\n",
      "        [ 0.2723,  0.1501,  0.1777, -0.0608,  0.0209, -0.0566, -0.1187,  0.1044,\n",
      "          0.0284, -0.1925],\n",
      "        [ 0.2183, -0.0458, -0.0868,  0.2422,  0.2899,  0.0156,  0.2748,  0.1606,\n",
      "          0.1072,  0.2375],\n",
      "        [ 0.0581, -0.2766, -0.0214, -0.0603, -0.0570,  0.2813, -0.0172, -0.0519,\n",
      "          0.0022,  0.0754],\n",
      "        [ 0.0725,  0.0626,  0.1116,  0.0689, -0.0994, -0.1217, -0.2950, -0.0031,\n",
      "          0.2694,  0.2645],\n",
      "        [ 0.2746, -0.0389,  0.2735,  0.2441, -0.0763, -0.3111, -0.0373,  0.0636,\n",
      "          0.0860, -0.2441],\n",
      "        [ 0.0064, -0.2285, -0.1898, -0.1411, -0.1861, -0.0096,  0.1285,  0.2219,\n",
      "          0.1662, -0.0157],\n",
      "        [-0.0747, -0.2296,  0.0851, -0.1734, -0.2557,  0.3007, -0.1054,  0.2185,\n",
      "          0.2145, -0.0635],\n",
      "        [ 0.2970,  0.2983, -0.3039,  0.1446, -0.3140,  0.2920,  0.0456,  0.1560,\n",
      "         -0.3033, -0.1732],\n",
      "        [ 0.0781, -0.1572,  0.2380, -0.0472,  0.3029, -0.2903, -0.2456,  0.1636,\n",
      "         -0.2085, -0.1547],\n",
      "        [ 0.0918,  0.2721,  0.0193,  0.1818,  0.2572,  0.2404, -0.1114,  0.0277,\n",
      "          0.1991,  0.0253],\n",
      "        [-0.2932,  0.1860,  0.1188, -0.1237, -0.2008, -0.3025,  0.2590,  0.2382,\n",
      "         -0.0350, -0.1825],\n",
      "        [-0.0509,  0.1859, -0.0547, -0.2042, -0.1036,  0.1270,  0.1419, -0.2718,\n",
      "          0.1478, -0.0970],\n",
      "        [ 0.2410, -0.2508,  0.0375,  0.1061,  0.0459, -0.1370,  0.0766,  0.1059,\n",
      "          0.1208, -0.1475],\n",
      "        [ 0.2632, -0.3035, -0.2031,  0.2132, -0.0526, -0.0996, -0.2872, -0.1260,\n",
      "          0.1635, -0.1884],\n",
      "        [-0.0465, -0.1336,  0.3068,  0.0023,  0.3158,  0.1364, -0.2522, -0.2140,\n",
      "          0.2424, -0.0345],\n",
      "        [ 0.1399,  0.0352,  0.1466,  0.1052, -0.2308, -0.2266, -0.2135, -0.0760,\n",
      "         -0.1018,  0.2151],\n",
      "        [ 0.0167,  0.2342, -0.0384, -0.0987,  0.1121, -0.0773, -0.0085, -0.2762,\n",
      "          0.0664,  0.1935],\n",
      "        [ 0.2428, -0.0141,  0.0086, -0.2101,  0.0736,  0.0498, -0.0705,  0.0904,\n",
      "         -0.0031, -0.0033],\n",
      "        [-0.1832, -0.1448,  0.0237,  0.1667,  0.0632,  0.1030, -0.0926, -0.1052,\n",
      "         -0.3168, -0.1327],\n",
      "        [-0.2092,  0.1738, -0.2021, -0.0993,  0.1040, -0.0987, -0.0446,  0.1755,\n",
      "          0.1895,  0.3026],\n",
      "        [-0.2082,  0.0902,  0.0462, -0.1179, -0.1734, -0.1183,  0.0364,  0.1524,\n",
      "          0.1155, -0.2593],\n",
      "        [ 0.1292,  0.1977,  0.0829, -0.0960, -0.1147, -0.0841,  0.0449, -0.2928,\n",
      "         -0.2713,  0.2090],\n",
      "        [ 0.2754,  0.1924,  0.1055, -0.3104,  0.2113,  0.1109, -0.2185, -0.0219,\n",
      "         -0.0231,  0.0810],\n",
      "        [-0.0768,  0.1871,  0.2311,  0.2996, -0.2936,  0.0854,  0.1941,  0.1455,\n",
      "         -0.0419,  0.2741],\n",
      "        [-0.1068, -0.0813,  0.1485,  0.2701,  0.2544, -0.2536, -0.1561, -0.1533,\n",
      "          0.1819, -0.0440],\n",
      "        [ 0.0774, -0.2414,  0.2974, -0.1069, -0.2671,  0.0359,  0.0638, -0.0036,\n",
      "         -0.0826,  0.2264],\n",
      "        [-0.1613,  0.2212, -0.2752,  0.1842, -0.0741,  0.3236,  0.2209,  0.2583,\n",
      "          0.2525,  0.1071],\n",
      "        [ 0.0753, -0.0152,  0.0225, -0.0895, -0.2236,  0.2044, -0.0826,  0.0828,\n",
      "         -0.0624,  0.3090],\n",
      "        [-0.0958,  0.2934, -0.2066,  0.1436,  0.3043, -0.1578,  0.2110,  0.0016,\n",
      "         -0.0546, -0.1855],\n",
      "        [ 0.2702, -0.3058, -0.1253,  0.1570, -0.1868, -0.1908,  0.2847, -0.1948,\n",
      "          0.0488, -0.3051],\n",
      "        [-0.1609, -0.2001,  0.0810,  0.1916, -0.0171,  0.0852,  0.1612, -0.1006,\n",
      "         -0.2109,  0.1944],\n",
      "        [ 0.1038,  0.1784,  0.3120, -0.2651,  0.2889, -0.2975,  0.1589,  0.0753,\n",
      "          0.0799,  0.0479],\n",
      "        [-0.2891, -0.1799,  0.0698, -0.0096, -0.2280,  0.0819, -0.2850, -0.0004,\n",
      "          0.1457, -0.3094],\n",
      "        [ 0.2623,  0.1318,  0.2120, -0.0138,  0.2300, -0.0343,  0.0540,  0.1337,\n",
      "         -0.2589, -0.0809],\n",
      "        [ 0.1975,  0.1834,  0.0598, -0.2470, -0.1967,  0.3081, -0.1126,  0.2393,\n",
      "         -0.2513,  0.1264],\n",
      "        [ 0.0700,  0.3149,  0.2739,  0.1935,  0.2828, -0.0033,  0.0730,  0.0219,\n",
      "          0.1632, -0.0230],\n",
      "        [-0.1750,  0.2818, -0.1145,  0.0220, -0.0951,  0.0697,  0.0769,  0.1637,\n",
      "          0.1466, -0.2177],\n",
      "        [-0.0092, -0.2237,  0.2997,  0.2982, -0.1221, -0.2378, -0.2617,  0.2705,\n",
      "         -0.1801,  0.2027],\n",
      "        [ 0.2579,  0.0643,  0.1955,  0.2455, -0.2076, -0.0059,  0.1510,  0.2529,\n",
      "         -0.0356,  0.2311],\n",
      "        [-0.2007, -0.1020,  0.0481, -0.1144, -0.1572,  0.0342,  0.2170, -0.2722,\n",
      "         -0.2384, -0.2138],\n",
      "        [-0.0289, -0.0702, -0.0999,  0.2120,  0.2051, -0.2052, -0.2429, -0.0476,\n",
      "         -0.2127, -0.2123],\n",
      "        [ 0.1645, -0.0883,  0.1392, -0.2695,  0.0527, -0.1560,  0.1799,  0.0325,\n",
      "         -0.2489,  0.0007],\n",
      "        [-0.2690,  0.2777, -0.3070,  0.2841,  0.1150,  0.1437, -0.3067, -0.2156,\n",
      "         -0.2006, -0.2120],\n",
      "        [-0.0671,  0.2026, -0.2980, -0.0839,  0.2058, -0.0719,  0.0372,  0.2026,\n",
      "          0.1822, -0.3100],\n",
      "        [ 0.2541,  0.0179,  0.1774, -0.0238,  0.1415, -0.2737, -0.0448,  0.2231,\n",
      "         -0.2230,  0.0200],\n",
      "        [-0.1197,  0.2828, -0.1782,  0.0165, -0.2743,  0.1548, -0.0942, -0.2538,\n",
      "          0.0553,  0.0795]]), 'fc1.bias': tensor([-0.1211,  0.0148,  0.0378,  0.2120, -0.0413,  0.1078,  0.1217,  0.1700,\n",
      "         0.0570,  0.0143, -0.1204,  0.1273,  0.3029,  0.0832,  0.1496, -0.3070,\n",
      "        -0.0420, -0.1628,  0.2380,  0.2512, -0.0870, -0.0623, -0.0734, -0.1873,\n",
      "        -0.1965, -0.1507, -0.1256, -0.2728,  0.0560,  0.1510,  0.0036, -0.0980,\n",
      "        -0.0578,  0.1158,  0.2380, -0.2130,  0.1396,  0.1255, -0.0770, -0.1450,\n",
      "         0.1544,  0.1864, -0.0724,  0.1837, -0.1364, -0.2292, -0.1437,  0.2473,\n",
      "        -0.3051, -0.1837]), 'batch_norm.weight': tensor([0.9991, 0.9956, 0.9984, 0.9965, 0.9988, 0.9988, 0.9986, 0.9961, 0.9920,\n",
      "        0.9988, 0.9987, 0.9973, 0.9932, 0.9984, 0.9934, 0.9946, 1.0022, 0.9944,\n",
      "        0.9963, 0.9988, 0.9939, 0.9979, 0.9963, 0.9991, 0.9958, 0.9949, 0.9990,\n",
      "        0.9937, 0.9956, 0.9953, 0.9942, 0.9959, 0.9985, 0.9949, 0.9981, 0.9931,\n",
      "        0.9923, 0.9945, 0.9945, 0.9982, 0.9903, 0.9949, 0.9932, 0.9939, 0.9991,\n",
      "        0.9972, 0.9993, 0.9993, 0.9978, 0.9953]), 'batch_norm.bias': tensor([-0.0003, -0.0071, -0.0027, -0.0032,  0.0008, -0.0002, -0.0019,  0.0018,\n",
      "         0.0061, -0.0006, -0.0023, -0.0026,  0.0060, -0.0021,  0.0069, -0.0045,\n",
      "        -0.0022,  0.0033,  0.0027, -0.0012,  0.0065,  0.0021,  0.0049,  0.0010,\n",
      "        -0.0054, -0.0041, -0.0001, -0.0040,  0.0034,  0.0073,  0.0070, -0.0041,\n",
      "        -0.0016,  0.0063, -0.0009, -0.0058,  0.0044, -0.0040,  0.0080, -0.0023,\n",
      "        -0.0076,  0.0037, -0.0050, -0.0049, -0.0040,  0.0042, -0.0019, -0.0009,\n",
      "        -0.0022,  0.0057]), 'batch_norm.running_mean': tensor([0.1839, 0.2544, 0.2050, 0.3884, 0.1550, 0.3402, 0.2521, 0.3113, 0.2604,\n",
      "        0.2011, 0.1778, 0.4032, 0.4193, 0.2837, 0.2890, 0.0702, 0.1782, 0.2054,\n",
      "        0.3810, 0.3329, 0.1405, 0.1350, 0.1272, 0.1431, 0.0840, 0.1328, 0.1931,\n",
      "        0.1271, 0.2553, 0.2842, 0.2823, 0.1438, 0.2301, 0.3804, 0.3120, 0.1699,\n",
      "        0.2613, 0.2820, 0.2181, 0.1657, 0.2659, 0.3402, 0.1976, 0.2990, 0.1424,\n",
      "        0.0931, 0.2171, 0.4120, 0.0899, 0.1266]), 'batch_norm.running_var': tensor([0.1067, 0.1397, 0.0882, 0.1809, 0.0634, 0.1602, 0.0744, 0.1433, 0.1224,\n",
      "        0.0761, 0.0886, 0.2585, 0.2079, 0.1320, 0.1612, 0.0274, 0.0584, 0.0958,\n",
      "        0.1931, 0.1359, 0.0597, 0.0436, 0.0513, 0.0751, 0.0326, 0.0654, 0.1032,\n",
      "        0.0773, 0.1230, 0.1339, 0.1642, 0.0613, 0.1283, 0.2239, 0.1134, 0.1019,\n",
      "        0.1240, 0.1201, 0.1318, 0.0830, 0.1009, 0.1849, 0.1003, 0.1436, 0.0678,\n",
      "        0.0385, 0.1423, 0.1890, 0.0407, 0.0575]), 'batch_norm.num_batches_tracked': tensor(160), 'fc2.weight': tensor([[-2.0947e-02, -9.3771e-02, -3.7354e-02, -3.4113e-02, -1.5081e-02,\n",
      "         -3.1806e-02, -2.4540e-02,  9.3603e-03,  3.1444e-02,  9.0439e-05,\n",
      "         -6.7787e-03,  3.2002e-02,  7.4018e-02, -9.7669e-03,  6.5410e-02,\n",
      "         -3.5562e-02, -8.8005e-02,  7.2207e-03, -1.3096e-02,  8.7284e-03,\n",
      "          4.8867e-02, -5.7978e-03,  3.5426e-02, -4.2898e-02, -5.8037e-02,\n",
      "         -2.2508e-02, -1.6533e-02, -8.5152e-03, -1.9491e-02,  1.1168e-01,\n",
      "          9.1236e-02, -6.2677e-02,  3.2548e-02,  6.7610e-02,  8.5043e-03,\n",
      "         -6.2358e-02,  6.6322e-02, -4.9547e-02,  1.0249e-01,  3.1188e-02,\n",
      "         -5.1755e-02,  3.2463e-02, -4.3899e-02, -4.2352e-02, -7.1319e-02,\n",
      "          3.0172e-02, -4.0505e-02,  3.8722e-02, -2.0223e-02,  4.3903e-02]]), 'fc2.bias': tensor([-0.0302])})\n"
     ]
    }
   ],
   "source": [
    "# Print the state_dict of the model\n",
    "print(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Name: fc1.weight Parameters:torch.Size([50, 10])\n",
      "Layer Name: fc1.bias Parameters:torch.Size([50])\n",
      "Layer Name: batch_norm.weight Parameters:torch.Size([50])\n",
      "Layer Name: batch_norm.bias Parameters:torch.Size([50])\n",
      "Layer Name: batch_norm.running_mean Parameters:torch.Size([50])\n",
      "Layer Name: batch_norm.running_var Parameters:torch.Size([50])\n",
      "Layer Name: batch_norm.num_batches_tracked Parameters:torch.Size([])\n",
      "Layer Name: fc2.weight Parameters:torch.Size([1, 50])\n",
      "Layer Name: fc2.bias Parameters:torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "# Print the paramters of each layer\n",
    "for k, v in model.state_dict().items():\n",
    "    print(f\"Layer Name: {k} Parameters:{v.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'state': {}, 'param_groups': [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5]}]}\n"
     ]
    }
   ],
   "source": [
    "# Print the hyperparameters of the Optimizer\n",
    "print(optimizer.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the state_dict for each (recommended approach)\n",
    "import torch\n",
    "\n",
    "torch.save(model.state_dict(), \"model_state_dict.pt\") # .pt or pth extension for models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the state_dict for the Optimizer\n",
    "torch.save(optimizer.state_dict(), \"optimizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: state_dict is ONLY saving the parameters!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Inference\n",
    "REVIEW: Inference is the process of using a trained model to make predictions.\n",
    "\n",
    "Let's load a model using using its state_dict and prepare it for inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FakeNet(\n",
      "  (fc1): Linear(in_features=10, out_features=50, bias=True)\n",
      "  (batch_norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=50, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create a new model\n",
    "new_model = FakeNet()\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Name: fc1.weight Parameters:tensor([[ 0.1562, -0.0331, -0.2115, -0.2645, -0.1664, -0.0214,  0.3064, -0.1800,\n",
      "          0.2725,  0.0491],\n",
      "        [-0.0186, -0.0935,  0.1419, -0.0618, -0.1698,  0.0124, -0.0262,  0.2673,\n",
      "         -0.1691, -0.0201],\n",
      "        [-0.1629,  0.0486,  0.1208, -0.0841,  0.0433,  0.1216, -0.3088, -0.0044,\n",
      "         -0.1457,  0.2384],\n",
      "        [-0.2759, -0.1889, -0.0628,  0.1217, -0.0447,  0.1648, -0.1095, -0.2860,\n",
      "          0.2170,  0.0545],\n",
      "        [-0.2098, -0.0973,  0.1577, -0.0325,  0.0589, -0.1601, -0.2941,  0.0223,\n",
      "          0.1271,  0.2688],\n",
      "        [-0.1858,  0.1936,  0.2691,  0.1475,  0.2413,  0.0023, -0.2949, -0.1399,\n",
      "         -0.2384,  0.2677],\n",
      "        [-0.0810,  0.2775,  0.1493, -0.2103, -0.0083, -0.1030, -0.2814,  0.0369,\n",
      "          0.1512, -0.1343],\n",
      "        [ 0.0107, -0.0862, -0.1850, -0.0260,  0.1453, -0.1932,  0.2639, -0.1293,\n",
      "         -0.1102,  0.0199],\n",
      "        [-0.2460,  0.1502, -0.1030,  0.0102, -0.1603, -0.2061,  0.1597,  0.1906,\n",
      "          0.1696, -0.0286],\n",
      "        [-0.1647,  0.2790, -0.2260, -0.1830,  0.2390, -0.3086, -0.3100,  0.1644,\n",
      "          0.0115,  0.1764],\n",
      "        [ 0.2976, -0.2768, -0.2111,  0.2844,  0.0772,  0.0631,  0.2773,  0.0163,\n",
      "         -0.1949,  0.1530],\n",
      "        [-0.2544,  0.1030,  0.2052, -0.2238, -0.2525, -0.1104, -0.2057, -0.0999,\n",
      "         -0.1152, -0.2005],\n",
      "        [-0.1832,  0.0518, -0.2606,  0.0596, -0.1538, -0.0739, -0.2397, -0.1015,\n",
      "         -0.1785,  0.0612],\n",
      "        [-0.2378,  0.0575,  0.0306, -0.1208,  0.2250,  0.2320,  0.0403, -0.1050,\n",
      "         -0.1159,  0.0938],\n",
      "        [ 0.2460, -0.0620, -0.0824, -0.1143, -0.0971, -0.2003, -0.0155,  0.1909,\n",
      "          0.1397, -0.2061],\n",
      "        [-0.2766,  0.1501,  0.1168,  0.2578,  0.2368,  0.0798,  0.0012, -0.2441,\n",
      "         -0.0262, -0.1670],\n",
      "        [-0.1151,  0.3118,  0.0847, -0.1542, -0.1354,  0.1560,  0.0469, -0.2912,\n",
      "          0.2549,  0.0661],\n",
      "        [ 0.1896,  0.0148,  0.0754, -0.0151, -0.1069,  0.0594, -0.0232, -0.2391,\n",
      "          0.2637,  0.0885],\n",
      "        [-0.1543,  0.2551, -0.0022, -0.2652,  0.2724,  0.2339, -0.2775, -0.0307,\n",
      "          0.0627,  0.1894],\n",
      "        [ 0.2994, -0.2239,  0.1103,  0.2626,  0.2949, -0.0172,  0.0934, -0.1195,\n",
      "          0.1910,  0.3034],\n",
      "        [-0.2090, -0.2296, -0.2080,  0.2909, -0.0335,  0.0563, -0.0816,  0.1229,\n",
      "         -0.1027,  0.0847],\n",
      "        [ 0.2608,  0.2837,  0.1662,  0.2166, -0.3120,  0.3059,  0.0810,  0.2266,\n",
      "          0.0551,  0.1622],\n",
      "        [ 0.3001,  0.0618,  0.0467, -0.1892, -0.1436, -0.2299, -0.2319,  0.2505,\n",
      "          0.2654, -0.1947],\n",
      "        [-0.1480, -0.0901,  0.1328,  0.2319,  0.0182, -0.1831, -0.2320,  0.1919,\n",
      "          0.2574,  0.1575],\n",
      "        [-0.1625,  0.1937,  0.1904,  0.1159, -0.3003, -0.0572,  0.1106, -0.2878,\n",
      "          0.2701,  0.2270],\n",
      "        [ 0.2737, -0.2996, -0.1625,  0.2518,  0.0952, -0.1513,  0.2104, -0.1102,\n",
      "         -0.0016,  0.0876],\n",
      "        [ 0.2689, -0.1600, -0.0932, -0.2724,  0.0971, -0.1769,  0.0326,  0.2234,\n",
      "         -0.0161, -0.0016],\n",
      "        [-0.0061, -0.2141,  0.2622,  0.2714,  0.2656, -0.2461,  0.1426, -0.1671,\n",
      "          0.3139,  0.1189],\n",
      "        [-0.2563, -0.1503,  0.2627,  0.2744, -0.0356, -0.1118,  0.1384,  0.1994,\n",
      "         -0.2905,  0.2911],\n",
      "        [-0.1368,  0.1740, -0.0362,  0.1302, -0.1511,  0.0953,  0.0056,  0.1548,\n",
      "          0.2112, -0.0397],\n",
      "        [-0.1549,  0.1122, -0.1919, -0.1702, -0.0223,  0.2770,  0.0215, -0.2157,\n",
      "          0.2116,  0.2812],\n",
      "        [ 0.2585, -0.1276,  0.1358, -0.2748, -0.0376,  0.1774,  0.0995, -0.2975,\n",
      "          0.1704,  0.2051],\n",
      "        [ 0.2929,  0.2966, -0.1179,  0.1096,  0.0572,  0.0185,  0.1115,  0.1630,\n",
      "          0.2535,  0.1914],\n",
      "        [-0.2421,  0.1406,  0.0159,  0.1875, -0.2966, -0.2469, -0.1941,  0.2218,\n",
      "         -0.1412,  0.0261],\n",
      "        [ 0.1171,  0.0443, -0.0926,  0.0599,  0.2303,  0.2561,  0.1124, -0.2001,\n",
      "          0.0633, -0.2001],\n",
      "        [-0.0209, -0.1424,  0.0338,  0.2217,  0.2888,  0.2788,  0.1691, -0.1097,\n",
      "          0.1208,  0.1185],\n",
      "        [-0.1622, -0.2848,  0.0408, -0.2805, -0.1198, -0.0107, -0.2866, -0.0112,\n",
      "         -0.1315,  0.2278],\n",
      "        [-0.3130,  0.0745, -0.1889,  0.1635,  0.2713, -0.0013, -0.1624,  0.1388,\n",
      "         -0.1092,  0.0026],\n",
      "        [-0.0014,  0.1813, -0.2202,  0.0167, -0.1803,  0.2102, -0.0563,  0.1557,\n",
      "          0.0212,  0.2933],\n",
      "        [-0.2497, -0.2508, -0.1061, -0.0048,  0.1726, -0.1806, -0.0218, -0.0797,\n",
      "          0.2262, -0.2365],\n",
      "        [ 0.0691, -0.2441,  0.1752,  0.2046,  0.2008,  0.0828, -0.0228,  0.0069,\n",
      "          0.1319,  0.2637],\n",
      "        [ 0.0728,  0.2052,  0.2385,  0.2749,  0.1277, -0.0877,  0.0615, -0.2473,\n",
      "          0.1863,  0.2908],\n",
      "        [ 0.0619, -0.3093,  0.2375,  0.1281, -0.1270,  0.2385,  0.1978,  0.1521,\n",
      "          0.1779, -0.2214],\n",
      "        [ 0.2277,  0.1653,  0.0348, -0.0074,  0.1495, -0.0879, -0.0863, -0.2271,\n",
      "          0.2520, -0.2372],\n",
      "        [ 0.2215,  0.1376, -0.1405, -0.1319, -0.1889,  0.3136, -0.2508,  0.3121,\n",
      "          0.2440, -0.2177],\n",
      "        [ 0.2908, -0.2762,  0.0235,  0.0310, -0.2421, -0.2899,  0.0515, -0.2356,\n",
      "          0.2115, -0.1062],\n",
      "        [-0.3053,  0.1252,  0.3154,  0.2358, -0.0244, -0.1592, -0.2458,  0.1350,\n",
      "          0.0029, -0.2930],\n",
      "        [-0.0895, -0.1081,  0.2508, -0.3053, -0.0951,  0.2741, -0.1519,  0.2246,\n",
      "         -0.0668,  0.0869],\n",
      "        [ 0.0684,  0.2990, -0.0777,  0.1110,  0.0060,  0.2711, -0.2777,  0.3099,\n",
      "          0.2919, -0.2256],\n",
      "        [ 0.0956, -0.3126,  0.1177,  0.1601, -0.1385, -0.1571, -0.1847,  0.1362,\n",
      "         -0.0987, -0.2668]])\n",
      "Layer Name: fc1.bias Parameters:tensor([-0.0160, -0.2227, -0.1778,  0.2211, -0.2041, -0.0875, -0.2516, -0.2328,\n",
      "         0.2934,  0.0311, -0.2670,  0.1401, -0.2254, -0.1124, -0.1853,  0.1248,\n",
      "         0.1207, -0.0818, -0.2334,  0.2143,  0.1003,  0.2379,  0.0966,  0.1104,\n",
      "         0.2180, -0.0393,  0.2175,  0.1179,  0.0538,  0.0672,  0.1466,  0.1448,\n",
      "         0.0100,  0.0828,  0.0073,  0.2274,  0.2399,  0.0432,  0.1520,  0.1651,\n",
      "        -0.3094,  0.0652,  0.2213,  0.0228,  0.0828, -0.0670,  0.1182, -0.2156,\n",
      "        -0.0267,  0.0371])\n",
      "Layer Name: batch_norm.weight Parameters:tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Layer Name: batch_norm.bias Parameters:tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.])\n",
      "Layer Name: batch_norm.running_mean Parameters:tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.])\n",
      "Layer Name: batch_norm.running_var Parameters:tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n",
      "Layer Name: batch_norm.num_batches_tracked Parameters:0\n",
      "Layer Name: fc2.weight Parameters:tensor([[-0.0984, -0.0550,  0.1374,  0.1398, -0.1097,  0.0176,  0.0856,  0.0175,\n",
      "         -0.0900, -0.0680,  0.1268, -0.0292,  0.1067,  0.0246, -0.0773, -0.0189,\n",
      "         -0.0979,  0.0390, -0.1407, -0.0575,  0.1387, -0.0157, -0.1164,  0.0654,\n",
      "         -0.1122,  0.1068, -0.1348, -0.0078, -0.1037, -0.1125, -0.0167,  0.0593,\n",
      "         -0.1367,  0.0002, -0.1398, -0.1005,  0.0443,  0.0333,  0.0432,  0.0620,\n",
      "         -0.0790, -0.1000, -0.1155, -0.0860,  0.1399,  0.0892, -0.0902,  0.1401,\n",
      "          0.0529, -0.1169]])\n",
      "Layer Name: fc2.bias Parameters:tensor([-0.0424])\n"
     ]
    }
   ],
   "source": [
    "# Show the current state_dict\n",
    "for k, v in new_model.state_dict().items():\n",
    "    print(f\"Layer Name: {k} Parameters:{v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the parameters into our model\n",
    "new_model.load_state_dict(torch.load(\"model_state_dict.pt\", weights_only=True)) # ONLY the parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Name: fc1.weight Parameters:tensor([[-0.2281, -0.0470,  0.2462, -0.2436, -0.0946, -0.3025,  0.1570,  0.0669,\n",
      "          0.1495,  0.3131],\n",
      "        [ 0.1679, -0.3009,  0.3013,  0.1753,  0.0325, -0.1105, -0.2211, -0.2457,\n",
      "         -0.1122,  0.2106],\n",
      "        [-0.1523, -0.2123, -0.1113, -0.2280, -0.3060, -0.0035, -0.0418,  0.1317,\n",
      "         -0.2353, -0.0075],\n",
      "        [ 0.0966, -0.2417, -0.1267, -0.1652,  0.3085, -0.2116,  0.2211,  0.0841,\n",
      "         -0.1746, -0.1794],\n",
      "        [ 0.2723,  0.1501,  0.1777, -0.0608,  0.0209, -0.0566, -0.1187,  0.1044,\n",
      "          0.0284, -0.1925],\n",
      "        [ 0.2183, -0.0458, -0.0868,  0.2422,  0.2899,  0.0156,  0.2748,  0.1606,\n",
      "          0.1072,  0.2375],\n",
      "        [ 0.0581, -0.2766, -0.0214, -0.0603, -0.0570,  0.2813, -0.0172, -0.0519,\n",
      "          0.0022,  0.0754],\n",
      "        [ 0.0725,  0.0626,  0.1116,  0.0689, -0.0994, -0.1217, -0.2950, -0.0031,\n",
      "          0.2694,  0.2645],\n",
      "        [ 0.2746, -0.0389,  0.2735,  0.2441, -0.0763, -0.3111, -0.0373,  0.0636,\n",
      "          0.0860, -0.2441],\n",
      "        [ 0.0064, -0.2285, -0.1898, -0.1411, -0.1861, -0.0096,  0.1285,  0.2219,\n",
      "          0.1662, -0.0157],\n",
      "        [-0.0747, -0.2296,  0.0851, -0.1734, -0.2557,  0.3007, -0.1054,  0.2185,\n",
      "          0.2145, -0.0635],\n",
      "        [ 0.2970,  0.2983, -0.3039,  0.1446, -0.3140,  0.2920,  0.0456,  0.1560,\n",
      "         -0.3033, -0.1732],\n",
      "        [ 0.0781, -0.1572,  0.2380, -0.0472,  0.3029, -0.2903, -0.2456,  0.1636,\n",
      "         -0.2085, -0.1547],\n",
      "        [ 0.0918,  0.2721,  0.0193,  0.1818,  0.2572,  0.2404, -0.1114,  0.0277,\n",
      "          0.1991,  0.0253],\n",
      "        [-0.2932,  0.1860,  0.1188, -0.1237, -0.2008, -0.3025,  0.2590,  0.2382,\n",
      "         -0.0350, -0.1825],\n",
      "        [-0.0509,  0.1859, -0.0547, -0.2042, -0.1036,  0.1270,  0.1419, -0.2718,\n",
      "          0.1478, -0.0970],\n",
      "        [ 0.2410, -0.2508,  0.0375,  0.1061,  0.0459, -0.1370,  0.0766,  0.1059,\n",
      "          0.1208, -0.1475],\n",
      "        [ 0.2632, -0.3035, -0.2031,  0.2132, -0.0526, -0.0996, -0.2872, -0.1260,\n",
      "          0.1635, -0.1884],\n",
      "        [-0.0465, -0.1336,  0.3068,  0.0023,  0.3158,  0.1364, -0.2522, -0.2140,\n",
      "          0.2424, -0.0345],\n",
      "        [ 0.1399,  0.0352,  0.1466,  0.1052, -0.2308, -0.2266, -0.2135, -0.0760,\n",
      "         -0.1018,  0.2151],\n",
      "        [ 0.0167,  0.2342, -0.0384, -0.0987,  0.1121, -0.0773, -0.0085, -0.2762,\n",
      "          0.0664,  0.1935],\n",
      "        [ 0.2428, -0.0141,  0.0086, -0.2101,  0.0736,  0.0498, -0.0705,  0.0904,\n",
      "         -0.0031, -0.0033],\n",
      "        [-0.1832, -0.1448,  0.0237,  0.1667,  0.0632,  0.1030, -0.0926, -0.1052,\n",
      "         -0.3168, -0.1327],\n",
      "        [-0.2092,  0.1738, -0.2021, -0.0993,  0.1040, -0.0987, -0.0446,  0.1755,\n",
      "          0.1895,  0.3026],\n",
      "        [-0.2082,  0.0902,  0.0462, -0.1179, -0.1734, -0.1183,  0.0364,  0.1524,\n",
      "          0.1155, -0.2593],\n",
      "        [ 0.1292,  0.1977,  0.0829, -0.0960, -0.1147, -0.0841,  0.0449, -0.2928,\n",
      "         -0.2713,  0.2090],\n",
      "        [ 0.2754,  0.1924,  0.1055, -0.3104,  0.2113,  0.1109, -0.2185, -0.0219,\n",
      "         -0.0231,  0.0810],\n",
      "        [-0.0768,  0.1871,  0.2311,  0.2996, -0.2936,  0.0854,  0.1941,  0.1455,\n",
      "         -0.0419,  0.2741],\n",
      "        [-0.1068, -0.0813,  0.1485,  0.2701,  0.2544, -0.2536, -0.1561, -0.1533,\n",
      "          0.1819, -0.0440],\n",
      "        [ 0.0774, -0.2414,  0.2974, -0.1069, -0.2671,  0.0359,  0.0638, -0.0036,\n",
      "         -0.0826,  0.2264],\n",
      "        [-0.1613,  0.2212, -0.2752,  0.1842, -0.0741,  0.3236,  0.2209,  0.2583,\n",
      "          0.2525,  0.1071],\n",
      "        [ 0.0753, -0.0152,  0.0225, -0.0895, -0.2236,  0.2044, -0.0826,  0.0828,\n",
      "         -0.0624,  0.3090],\n",
      "        [-0.0958,  0.2934, -0.2066,  0.1436,  0.3043, -0.1578,  0.2110,  0.0016,\n",
      "         -0.0546, -0.1855],\n",
      "        [ 0.2702, -0.3058, -0.1253,  0.1570, -0.1868, -0.1908,  0.2847, -0.1948,\n",
      "          0.0488, -0.3051],\n",
      "        [-0.1609, -0.2001,  0.0810,  0.1916, -0.0171,  0.0852,  0.1612, -0.1006,\n",
      "         -0.2109,  0.1944],\n",
      "        [ 0.1038,  0.1784,  0.3120, -0.2651,  0.2889, -0.2975,  0.1589,  0.0753,\n",
      "          0.0799,  0.0479],\n",
      "        [-0.2891, -0.1799,  0.0698, -0.0096, -0.2280,  0.0819, -0.2850, -0.0004,\n",
      "          0.1457, -0.3094],\n",
      "        [ 0.2623,  0.1318,  0.2120, -0.0138,  0.2300, -0.0343,  0.0540,  0.1337,\n",
      "         -0.2589, -0.0809],\n",
      "        [ 0.1975,  0.1834,  0.0598, -0.2470, -0.1967,  0.3081, -0.1126,  0.2393,\n",
      "         -0.2513,  0.1264],\n",
      "        [ 0.0700,  0.3149,  0.2739,  0.1935,  0.2828, -0.0033,  0.0730,  0.0219,\n",
      "          0.1632, -0.0230],\n",
      "        [-0.1750,  0.2818, -0.1145,  0.0220, -0.0951,  0.0697,  0.0769,  0.1637,\n",
      "          0.1466, -0.2177],\n",
      "        [-0.0092, -0.2237,  0.2997,  0.2982, -0.1221, -0.2378, -0.2617,  0.2705,\n",
      "         -0.1801,  0.2027],\n",
      "        [ 0.2579,  0.0643,  0.1955,  0.2455, -0.2076, -0.0059,  0.1510,  0.2529,\n",
      "         -0.0356,  0.2311],\n",
      "        [-0.2007, -0.1020,  0.0481, -0.1144, -0.1572,  0.0342,  0.2170, -0.2722,\n",
      "         -0.2384, -0.2138],\n",
      "        [-0.0289, -0.0702, -0.0999,  0.2120,  0.2051, -0.2052, -0.2429, -0.0476,\n",
      "         -0.2127, -0.2123],\n",
      "        [ 0.1645, -0.0883,  0.1392, -0.2695,  0.0527, -0.1560,  0.1799,  0.0325,\n",
      "         -0.2489,  0.0007],\n",
      "        [-0.2690,  0.2777, -0.3070,  0.2841,  0.1150,  0.1437, -0.3067, -0.2156,\n",
      "         -0.2006, -0.2120],\n",
      "        [-0.0671,  0.2026, -0.2980, -0.0839,  0.2058, -0.0719,  0.0372,  0.2026,\n",
      "          0.1822, -0.3100],\n",
      "        [ 0.2541,  0.0179,  0.1774, -0.0238,  0.1415, -0.2737, -0.0448,  0.2231,\n",
      "         -0.2230,  0.0200],\n",
      "        [-0.1197,  0.2828, -0.1782,  0.0165, -0.2743,  0.1548, -0.0942, -0.2538,\n",
      "          0.0553,  0.0795]])\n",
      "Layer Name: fc1.bias Parameters:tensor([-0.1211,  0.0148,  0.0378,  0.2120, -0.0413,  0.1078,  0.1217,  0.1700,\n",
      "         0.0570,  0.0143, -0.1204,  0.1273,  0.3029,  0.0832,  0.1496, -0.3070,\n",
      "        -0.0420, -0.1628,  0.2380,  0.2512, -0.0870, -0.0623, -0.0734, -0.1873,\n",
      "        -0.1965, -0.1507, -0.1256, -0.2728,  0.0560,  0.1510,  0.0036, -0.0980,\n",
      "        -0.0578,  0.1158,  0.2380, -0.2130,  0.1396,  0.1255, -0.0770, -0.1450,\n",
      "         0.1544,  0.1864, -0.0724,  0.1837, -0.1364, -0.2292, -0.1437,  0.2473,\n",
      "        -0.3051, -0.1837])\n",
      "Layer Name: batch_norm.weight Parameters:tensor([0.9991, 0.9956, 0.9984, 0.9965, 0.9988, 0.9988, 0.9986, 0.9961, 0.9920,\n",
      "        0.9988, 0.9987, 0.9973, 0.9932, 0.9984, 0.9934, 0.9946, 1.0022, 0.9944,\n",
      "        0.9963, 0.9988, 0.9939, 0.9979, 0.9963, 0.9991, 0.9958, 0.9949, 0.9990,\n",
      "        0.9937, 0.9956, 0.9953, 0.9942, 0.9959, 0.9985, 0.9949, 0.9981, 0.9931,\n",
      "        0.9923, 0.9945, 0.9945, 0.9982, 0.9903, 0.9949, 0.9932, 0.9939, 0.9991,\n",
      "        0.9972, 0.9993, 0.9993, 0.9978, 0.9953])\n",
      "Layer Name: batch_norm.bias Parameters:tensor([-0.0003, -0.0071, -0.0027, -0.0032,  0.0008, -0.0002, -0.0019,  0.0018,\n",
      "         0.0061, -0.0006, -0.0023, -0.0026,  0.0060, -0.0021,  0.0069, -0.0045,\n",
      "        -0.0022,  0.0033,  0.0027, -0.0012,  0.0065,  0.0021,  0.0049,  0.0010,\n",
      "        -0.0054, -0.0041, -0.0001, -0.0040,  0.0034,  0.0073,  0.0070, -0.0041,\n",
      "        -0.0016,  0.0063, -0.0009, -0.0058,  0.0044, -0.0040,  0.0080, -0.0023,\n",
      "        -0.0076,  0.0037, -0.0050, -0.0049, -0.0040,  0.0042, -0.0019, -0.0009,\n",
      "        -0.0022,  0.0057])\n",
      "Layer Name: batch_norm.running_mean Parameters:tensor([0.1839, 0.2544, 0.2050, 0.3884, 0.1550, 0.3402, 0.2521, 0.3113, 0.2604,\n",
      "        0.2011, 0.1778, 0.4032, 0.4193, 0.2837, 0.2890, 0.0702, 0.1782, 0.2054,\n",
      "        0.3810, 0.3329, 0.1405, 0.1350, 0.1272, 0.1431, 0.0840, 0.1328, 0.1931,\n",
      "        0.1271, 0.2553, 0.2842, 0.2823, 0.1438, 0.2301, 0.3804, 0.3120, 0.1699,\n",
      "        0.2613, 0.2820, 0.2181, 0.1657, 0.2659, 0.3402, 0.1976, 0.2990, 0.1424,\n",
      "        0.0931, 0.2171, 0.4120, 0.0899, 0.1266])\n",
      "Layer Name: batch_norm.running_var Parameters:tensor([0.1067, 0.1397, 0.0882, 0.1809, 0.0634, 0.1602, 0.0744, 0.1433, 0.1224,\n",
      "        0.0761, 0.0886, 0.2585, 0.2079, 0.1320, 0.1612, 0.0274, 0.0584, 0.0958,\n",
      "        0.1931, 0.1359, 0.0597, 0.0436, 0.0513, 0.0751, 0.0326, 0.0654, 0.1032,\n",
      "        0.0773, 0.1230, 0.1339, 0.1642, 0.0613, 0.1283, 0.2239, 0.1134, 0.1019,\n",
      "        0.1240, 0.1201, 0.1318, 0.0830, 0.1009, 0.1849, 0.1003, 0.1436, 0.0678,\n",
      "        0.0385, 0.1423, 0.1890, 0.0407, 0.0575])\n",
      "Layer Name: batch_norm.num_batches_tracked Parameters:160\n",
      "Layer Name: fc2.weight Parameters:tensor([[-2.0947e-02, -9.3771e-02, -3.7354e-02, -3.4113e-02, -1.5081e-02,\n",
      "         -3.1806e-02, -2.4540e-02,  9.3603e-03,  3.1444e-02,  9.0439e-05,\n",
      "         -6.7787e-03,  3.2002e-02,  7.4018e-02, -9.7669e-03,  6.5410e-02,\n",
      "         -3.5562e-02, -8.8005e-02,  7.2207e-03, -1.3096e-02,  8.7284e-03,\n",
      "          4.8867e-02, -5.7978e-03,  3.5426e-02, -4.2898e-02, -5.8037e-02,\n",
      "         -2.2508e-02, -1.6533e-02, -8.5152e-03, -1.9491e-02,  1.1168e-01,\n",
      "          9.1236e-02, -6.2677e-02,  3.2548e-02,  6.7610e-02,  8.5043e-03,\n",
      "         -6.2358e-02,  6.6322e-02, -4.9547e-02,  1.0249e-01,  3.1188e-02,\n",
      "         -5.1755e-02,  3.2463e-02, -4.3899e-02, -4.2352e-02, -7.1319e-02,\n",
      "          3.0172e-02, -4.0505e-02,  3.8722e-02, -2.0223e-02,  4.3903e-02]])\n",
      "Layer Name: fc2.bias Parameters:tensor([-0.0302])\n"
     ]
    }
   ],
   "source": [
    "# Print it again to show the difference\n",
    "for k, v in new_model.state_dict().items():\n",
    "    print(f\"Layer Name: {k} Parameters:{v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The parameters have been updated after loaded!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1106,  1.7144,  0.1781,  2.1832,  0.6793, -0.4792,  0.3403, -1.2182,\n",
      "         -1.1033, -0.5374]])\n"
     ]
    }
   ],
   "source": [
    "# Create example input\n",
    "import torch\n",
    "# Random batch size of 1-10 features\n",
    "sample_input = torch.randn(1, 10)\n",
    "print(sample_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1387]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Lets do an example infernce on our model\n",
    "new_model.eval()\n",
    "\n",
    "# Call the model with input to get a prediction\n",
    "output = new_model(sample_input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Loading entire Model\n",
    "PyTorch provides the option to save a full model to the filesystem as well.\n",
    "\n",
    "full model = full python pickle version of model\n",
    "\n",
    "This can potentially cause issues because it relies on the exact class definitions and file structure from when the model was saved, so loading may fail if used in a different project or after code changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save a full model\n",
    "import torch\n",
    "\n",
    "torch.save(model, \"model_full.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FakeNet(\n",
      "  (fc1): Linear(in_features=10, out_features=50, bias=True)\n",
      "  (batch_norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=50, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Import the model class from file\n",
    "from fake_net import FakeNet\n",
    "\n",
    "# Initialize and use the model\n",
    "model = FakeNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try again\n",
    "torch.save(model, \"model_full.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--@ 1 JABERI  staff   7.9K Nov 12 22:27 model_full.pt\n",
      "-rw-r--r--@ 1 JABERI  staff   7.0K Nov 12 22:27 model_state_dict.pt\n"
     ]
    }
   ],
   "source": [
    "# Look at the size difference\n",
    "!ls -lh model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FakeNet(\n",
      "  (fc1): Linear(in_features=10, out_features=50, bias=True)\n",
      "  (batch_norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=50, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load a full model\n",
    "from fake_net import FakeNet\n",
    "\n",
    "# Initialize and use the model\n",
    "new_model = FakeNet()\n",
    "print(new_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FakeNet(\n",
      "  (fc1): Linear(in_features=10, out_features=50, bias=True)\n",
      "  (batch_norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=50, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load it from the full model\n",
    "new_model = torch.load(\"model_full.pt\", weights_only=False) # More than just the parameters\n",
    "print(new_model)\n",
    "\n",
    "#new_model = FakeNet()            # recreate your architecture\n",
    "#new_model.load_state_dict(torch.load(\"model_full.pt\", weights_only=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0808]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Check inference\n",
    "new_model.eval()\n",
    "\n",
    "# Call the model with input to get a prediction\n",
    "output = new_model(sample_input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Loading a Checkpoint\n",
    "A model checkpoint is a way to save parameters as a snapshot in a point in time. \n",
    "\n",
    "This is helpful to continue a long training job that may have failed at some point or to give multiple models as options to use from a training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save a checkpoint\n",
    "import torch\n",
    "\n",
    "# dummy epoch and loss\n",
    "epoch = 5\n",
    "loss = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define checkpoint path\n",
    "checkpoint_path = f\"{epoch}_checkpoint.tar\"\n",
    "# Save a checkpoint\n",
    "torch.save({'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss}, \n",
    "            checkpoint_path) # .tar file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Checkpoint saved successfully: 5_checkpoint.tar\n"
     ]
    }
   ],
   "source": [
    "# Print confirmation\n",
    "print(f\"✅ Checkpoint saved successfully: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Checkpoint saved successfully at: /Users/JABERI/Downloads/PyTorch-main/5_checkpoint.tar\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Go up 3 levels from the current folder to reach the project root\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), \"../../..\"))\n",
    "\n",
    "# Define checkpoint path in the project root\n",
    "checkpoint_path = os.path.join(project_root, f\"{epoch}_checkpoint.tar\")\n",
    "\n",
    "# Save checkpoint\n",
    "torch.save({\n",
    "    'epoch': epoch,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': loss\n",
    "}, checkpoint_path)\n",
    "\n",
    "# Print confirmation\n",
    "print(f\"✅ Checkpoint saved successfully at: {checkpoint_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FakeNet(\n",
      "  (fc1): Linear(in_features=10, out_features=50, bias=True)\n",
      "  (batch_norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=50, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Load the checkpoint\n",
    "# Initialize the Model as we have before. NOTE: also optimizer in our case\n",
    "from fake_net import FakeNet\n",
    "\n",
    "# Initialize and use the model\n",
    "model = FakeNet()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model as a checkpoint\n",
    "import torch\n",
    "\n",
    "# Load the tar file\n",
    "checkpoint = torch.load(f\"{epoch}_checkpoint.tar\", weights_only='true')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 5, 'model_state_dict': OrderedDict({'fc1.weight': tensor([[ 0.1939, -0.0690,  0.1930,  0.3073, -0.0985,  0.2643,  0.0314,  0.2465,\n",
      "         -0.0899, -0.1004],\n",
      "        [ 0.2517,  0.1523, -0.1693, -0.0171, -0.2050,  0.2094,  0.0029, -0.1450,\n",
      "          0.0632,  0.3087],\n",
      "        [-0.1711, -0.1093, -0.0354, -0.2856,  0.0973, -0.2452, -0.0832,  0.1497,\n",
      "         -0.2379,  0.2537],\n",
      "        [-0.2583,  0.1652,  0.0819,  0.2358,  0.1571, -0.0864,  0.0800,  0.1662,\n",
      "          0.1451, -0.1171],\n",
      "        [ 0.1074, -0.2723, -0.0270, -0.3016, -0.1467,  0.2596,  0.3072,  0.1203,\n",
      "         -0.2644, -0.0127],\n",
      "        [-0.0597,  0.1511,  0.0701, -0.2397,  0.1183, -0.2840, -0.1997, -0.1482,\n",
      "         -0.2438,  0.2162],\n",
      "        [ 0.0460, -0.2512,  0.3027, -0.2167, -0.1438, -0.2287,  0.2517, -0.1988,\n",
      "         -0.1735,  0.1271],\n",
      "        [ 0.0013, -0.0672,  0.1027, -0.1696, -0.1584,  0.1351,  0.0048,  0.1264,\n",
      "         -0.2722,  0.0732],\n",
      "        [-0.0551, -0.0351, -0.2239,  0.0305, -0.1977, -0.2706,  0.2557,  0.1197,\n",
      "         -0.2606, -0.1070],\n",
      "        [ 0.1454, -0.1536, -0.0184,  0.2133, -0.2488,  0.0354,  0.0989, -0.2397,\n",
      "          0.0509,  0.2059],\n",
      "        [-0.1151, -0.0461,  0.0541,  0.2165,  0.2133, -0.3099, -0.2566,  0.2248,\n",
      "          0.2916, -0.2846],\n",
      "        [ 0.1889, -0.1310,  0.0832, -0.0295, -0.0672,  0.2267, -0.2923, -0.0890,\n",
      "         -0.2984,  0.1180],\n",
      "        [-0.2390, -0.0744,  0.3043,  0.1276, -0.0527, -0.1234,  0.0562, -0.2973,\n",
      "         -0.0393, -0.2902],\n",
      "        [-0.0846,  0.1124,  0.2051, -0.0652, -0.1412, -0.0415, -0.1473, -0.1369,\n",
      "         -0.3150, -0.1636],\n",
      "        [ 0.1397, -0.2194,  0.0939, -0.0903, -0.0848,  0.2202,  0.0945, -0.1175,\n",
      "         -0.0775,  0.1805],\n",
      "        [ 0.1968, -0.0228,  0.1326,  0.1389,  0.0630, -0.1575, -0.0722,  0.1168,\n",
      "          0.0635,  0.1755],\n",
      "        [ 0.0972, -0.0988,  0.1701,  0.2508, -0.1696,  0.2992, -0.1266, -0.3038,\n",
      "          0.1649,  0.2154],\n",
      "        [ 0.1568, -0.1077, -0.2240,  0.2225, -0.1928,  0.0301, -0.2182,  0.1588,\n",
      "         -0.1817,  0.2310],\n",
      "        [-0.1291, -0.2647, -0.0902,  0.2159,  0.0522,  0.1070,  0.0885,  0.3009,\n",
      "          0.2213,  0.2148],\n",
      "        [-0.0259,  0.2923,  0.0017,  0.0325, -0.2842, -0.0594, -0.1670,  0.2822,\n",
      "          0.2327, -0.0447],\n",
      "        [ 0.1287, -0.2356,  0.1112, -0.0209,  0.1663, -0.0230, -0.0896, -0.1034,\n",
      "         -0.0342, -0.3132],\n",
      "        [-0.1045,  0.2159, -0.2594, -0.2901, -0.3112,  0.0459, -0.1566, -0.2712,\n",
      "         -0.2956, -0.2542],\n",
      "        [ 0.1122, -0.3096, -0.1531,  0.2745,  0.0656,  0.0765,  0.1490,  0.0050,\n",
      "          0.0908, -0.2012],\n",
      "        [ 0.2582, -0.1905, -0.1979,  0.1277, -0.1813,  0.2864,  0.2199,  0.0420,\n",
      "         -0.1437,  0.1551],\n",
      "        [ 0.1416, -0.2187,  0.0098, -0.2254,  0.1699, -0.2842, -0.1211,  0.0389,\n",
      "         -0.1573,  0.1224],\n",
      "        [-0.1421,  0.0537, -0.1371,  0.0460,  0.0637,  0.2497, -0.2370,  0.3006,\n",
      "          0.0846, -0.1528],\n",
      "        [-0.2289, -0.1292, -0.0544, -0.2001,  0.1553,  0.0291,  0.0302,  0.2420,\n",
      "          0.0926, -0.2640],\n",
      "        [-0.2404, -0.1659, -0.0351,  0.0290,  0.0925,  0.1620,  0.1424,  0.0998,\n",
      "         -0.1087,  0.1742],\n",
      "        [-0.0194, -0.2150, -0.2124,  0.1102, -0.2782,  0.2870,  0.2352, -0.1699,\n",
      "         -0.0164,  0.1076],\n",
      "        [-0.0678, -0.0428,  0.2196, -0.0590,  0.2353, -0.0624, -0.0059, -0.1567,\n",
      "         -0.1490,  0.1932],\n",
      "        [ 0.0183,  0.1114, -0.2449, -0.1782, -0.2074,  0.0166,  0.0640,  0.0399,\n",
      "          0.3003,  0.2147],\n",
      "        [ 0.0524, -0.0139,  0.1678, -0.2052,  0.1113, -0.1428,  0.1856, -0.0432,\n",
      "         -0.0030, -0.2002],\n",
      "        [-0.1154,  0.3010,  0.1848,  0.1269, -0.2437, -0.0802, -0.0693,  0.1527,\n",
      "         -0.1847, -0.0853],\n",
      "        [ 0.0677, -0.0779,  0.0912, -0.0330, -0.1218,  0.3093, -0.1764, -0.1210,\n",
      "         -0.0958, -0.2450],\n",
      "        [-0.0488, -0.2217,  0.1140, -0.2677,  0.1341,  0.1693, -0.0207, -0.2392,\n",
      "          0.0100, -0.0448],\n",
      "        [-0.1822,  0.1458, -0.1088,  0.1581, -0.0583, -0.2787,  0.3102,  0.2651,\n",
      "         -0.0734,  0.0742],\n",
      "        [ 0.0828,  0.0230, -0.2550, -0.0100,  0.3132, -0.1500,  0.0914, -0.1504,\n",
      "         -0.1501, -0.2402],\n",
      "        [ 0.1708, -0.2201, -0.1223,  0.1903, -0.1305,  0.1403, -0.1089,  0.2087,\n",
      "          0.3000, -0.2131],\n",
      "        [-0.2993, -0.0657,  0.0055, -0.0901, -0.1594, -0.2705, -0.0967, -0.2180,\n",
      "         -0.1719,  0.0961],\n",
      "        [-0.1711, -0.0611,  0.2702, -0.2550, -0.0607,  0.0592,  0.2320,  0.1507,\n",
      "         -0.0634, -0.2351],\n",
      "        [-0.2526,  0.1544,  0.2764, -0.1418,  0.2165, -0.2646, -0.0297,  0.1107,\n",
      "         -0.3126, -0.3074],\n",
      "        [-0.1821, -0.2366, -0.0253, -0.1169, -0.0488, -0.0774,  0.2592,  0.2260,\n",
      "         -0.2919,  0.1248],\n",
      "        [-0.0981, -0.0615,  0.1661, -0.1040,  0.1444,  0.2652,  0.0779, -0.2791,\n",
      "          0.0288, -0.1241],\n",
      "        [-0.1016,  0.2558,  0.2840, -0.1118, -0.0448, -0.0681,  0.2745,  0.0829,\n",
      "         -0.2213, -0.0070],\n",
      "        [ 0.1758,  0.2814,  0.0501,  0.0508,  0.1100,  0.1836, -0.0818,  0.2632,\n",
      "          0.0402,  0.2868],\n",
      "        [ 0.2912,  0.0426, -0.0030, -0.3104, -0.2929, -0.0422,  0.3068,  0.2938,\n",
      "         -0.0981,  0.0256],\n",
      "        [-0.2045,  0.2679, -0.2824,  0.0285,  0.1734, -0.0855, -0.2491,  0.0741,\n",
      "         -0.2117,  0.2920],\n",
      "        [-0.1279, -0.3087, -0.1730,  0.3081,  0.1328, -0.0722, -0.0557,  0.2858,\n",
      "         -0.0440, -0.2974],\n",
      "        [-0.0860,  0.1770, -0.0004, -0.0366,  0.2253, -0.1009,  0.2397, -0.2819,\n",
      "         -0.0021,  0.0792],\n",
      "        [-0.0594,  0.1927, -0.2523, -0.1355,  0.1644, -0.0347, -0.1056,  0.0559,\n",
      "         -0.1465,  0.1003]]), 'fc1.bias': tensor([-0.2153, -0.0312,  0.1183,  0.0989,  0.2584, -0.2400, -0.1480,  0.2302,\n",
      "        -0.1913, -0.0118, -0.1515,  0.1337, -0.1677,  0.2972,  0.2871,  0.0651,\n",
      "        -0.1522,  0.0566,  0.1098, -0.1668, -0.3027,  0.1324,  0.0781, -0.1269,\n",
      "         0.1912,  0.2790, -0.2830,  0.1802, -0.1888,  0.1695,  0.1112,  0.0342,\n",
      "        -0.2226, -0.2737,  0.2989,  0.2557, -0.0342, -0.0232,  0.1222,  0.2958,\n",
      "         0.1370,  0.1989,  0.1469, -0.1817, -0.1156,  0.2211, -0.0401, -0.2851,\n",
      "         0.1752, -0.2251]), 'batch_norm.weight': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'batch_norm.bias': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.]), 'batch_norm.running_mean': tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0.]), 'batch_norm.running_var': tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
      "        1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), 'batch_norm.num_batches_tracked': tensor(0), 'fc2.weight': tensor([[ 0.1308,  0.0605,  0.0201, -0.1156, -0.0360, -0.1227, -0.0594, -0.1316,\n",
      "          0.0671, -0.0350,  0.1080, -0.0617,  0.0741,  0.0032, -0.0690,  0.1187,\n",
      "          0.0363, -0.0724,  0.0024, -0.1267, -0.0683, -0.1149, -0.0868,  0.1174,\n",
      "         -0.0056,  0.0909,  0.0354, -0.0051,  0.0289,  0.1224,  0.1241,  0.0998,\n",
      "          0.0708,  0.1286, -0.1194,  0.0734,  0.0357, -0.1199, -0.0756, -0.1119,\n",
      "          0.0591, -0.0942, -0.0588, -0.0789,  0.1257,  0.0145,  0.0015, -0.0585,\n",
      "          0.0312, -0.0765]]), 'fc2.bias': tensor([-0.0492])}), 'optimizer_state_dict': {'state': {}, 'param_groups': [{'lr': 0.01, 'momentum': 0, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'maximize': False, 'foreach': None, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3, 4, 5]}]}, 'loss': 0.05}\n"
     ]
    }
   ],
   "source": [
    "# Show the checkpoint info\n",
    "print(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the parameters to our model\n",
    "model.load_state_dict(checkpoint['model_state_dict']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the optimizer\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05 5\n"
     ]
    }
   ],
   "source": [
    "# Load the loss and the epoch. NOTE that we could have save other information here as well\n",
    "loss = checkpoint['loss']\n",
    "epoch = checkpoint['epoch']\n",
    "print(loss, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0808]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Test inference\n",
    "model.eval()\n",
    "output = model(sample_input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Checkpoints to Training\n",
    "Its good practice to include checkpoints as part of your training loop.\n",
    "\n",
    "How you save checkpoints is up to you. ie: every so often, every epoch, every epoch which improves on loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets include a checkpoint in our training loop that saves a checkpoint every 2 epochs\n",
    "# Train a fake model\n",
    "N_EPOCHS = 10\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    for i, (inputs, targets) in enumerate(data_loader):\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    ######### Save a checkpoint every 2 epochs\n",
    "    if epoch % 2 == 0:\n",
    "        torch.save({'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss}, \n",
    "                f'training_checkpoint_{epoch}.tar')\n",
    "\n",
    "# Save the final checkpoint after the last epoch\n",
    "torch.save({\n",
    "    'epoch': N_EPOCHS,\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': loss\n",
    "}, 'training_checkpoint_final.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--@ 1 JABERI  staff  7717 Nov 12 22:27 training_checkpoint_0.tar\n",
      "-rw-r--r--@ 1 JABERI  staff  7717 Nov 12 22:27 training_checkpoint_2.tar\n",
      "-rw-r--r--@ 1 JABERI  staff  7717 Nov 12 22:27 training_checkpoint_4.tar\n",
      "-rw-r--r--@ 1 JABERI  staff  7717 Nov 12 22:27 training_checkpoint_6.tar\n",
      "-rw-r--r--@ 1 JABERI  staff  7717 Nov 12 22:27 training_checkpoint_8.tar\n",
      "-rw-r--r--@ 1 JABERI  staff  7845 Nov 12 22:27 training_checkpoint_final.tar\n"
     ]
    }
   ],
   "source": [
    "# List all the checkpoints\n",
    "!ls -l training_checkpoint*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: We can now load any of these checkpoints to either continue training from that point in time or run inference. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warmstarting\n",
    "Warmstarting is where we initialize a new model to train from trained parameters of a previously trained model.\n",
    "\n",
    "This is helpful in Transfer Learning which is covered in more detail later.\n",
    "\n",
    "With warmstarting we can also initialize only certain layers of a previously trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Fake Model\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FakeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FakeNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(10, 50)\n",
    "        self.batch_norm = nn.BatchNorm1d(50) \n",
    "        self.fc2 = nn.Linear(50, 1)        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))              \n",
    "        x = self.batch_norm(x)               \n",
    "        x = self.fc2(x)                      \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FakeNet(\n",
       "  (fc1): Linear(in_features=10, out_features=50, bias=True)\n",
       "  (batch_norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc2): Linear(in_features=50, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create new model\n",
    "new_model = FakeNet()\n",
    "new_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('fc1.weight',\n",
       "              tensor([[-0.1815, -0.2359,  0.1409,  0.2466,  0.3120,  0.0500,  0.2572,  0.2403,\n",
       "                       -0.1810, -0.1478],\n",
       "                      [-0.0199,  0.0262,  0.2856, -0.1054, -0.2220, -0.2707,  0.3002, -0.3129,\n",
       "                        0.0289, -0.1051],\n",
       "                      [ 0.0826,  0.2328, -0.1066,  0.0225, -0.2014, -0.1936,  0.1761,  0.1122,\n",
       "                        0.2779, -0.1517],\n",
       "                      [-0.0328,  0.1457,  0.2343, -0.3087, -0.2595,  0.0876,  0.0765,  0.1837,\n",
       "                        0.2121, -0.0769],\n",
       "                      [-0.1719,  0.2916,  0.1315,  0.2228, -0.0696,  0.2572, -0.2160, -0.0908,\n",
       "                        0.1333,  0.1842],\n",
       "                      [ 0.0148,  0.2191,  0.2171,  0.2507,  0.1419, -0.1592,  0.3144, -0.0986,\n",
       "                        0.0805, -0.1562],\n",
       "                      [ 0.3069,  0.2559, -0.0986,  0.0869, -0.2676, -0.1157, -0.0418, -0.2473,\n",
       "                       -0.3012, -0.1968],\n",
       "                      [-0.0417, -0.2555,  0.1788,  0.0704, -0.1491, -0.1099,  0.0372,  0.2077,\n",
       "                       -0.0229,  0.1089],\n",
       "                      [-0.0591, -0.2429,  0.0639,  0.1576, -0.0533, -0.0164, -0.2563,  0.2824,\n",
       "                       -0.0987,  0.0237],\n",
       "                      [-0.2162, -0.0664, -0.1692, -0.2886,  0.1752,  0.2130,  0.1880,  0.1845,\n",
       "                        0.0124, -0.2752],\n",
       "                      [ 0.1637, -0.2219, -0.2075, -0.0744, -0.1415, -0.0595, -0.0834, -0.2472,\n",
       "                       -0.0709, -0.1272],\n",
       "                      [ 0.1574, -0.2053, -0.1119, -0.1495, -0.2346, -0.0152,  0.2007,  0.1640,\n",
       "                        0.2899, -0.3023],\n",
       "                      [-0.0351,  0.0116,  0.0337, -0.2067, -0.0268,  0.1579, -0.1918,  0.1419,\n",
       "                        0.0486, -0.2450],\n",
       "                      [ 0.2988,  0.0057,  0.2168,  0.2073, -0.0492,  0.2852, -0.0455, -0.1060,\n",
       "                        0.0561,  0.1843],\n",
       "                      [ 0.1226, -0.1799, -0.1162, -0.3115,  0.1475, -0.0958, -0.0646, -0.3132,\n",
       "                       -0.1679, -0.2021],\n",
       "                      [ 0.2385,  0.1281,  0.2339,  0.0525, -0.1598, -0.0913, -0.1140, -0.0404,\n",
       "                       -0.3016,  0.0819],\n",
       "                      [ 0.0598, -0.2153, -0.2634,  0.0444, -0.0969,  0.2495, -0.1622, -0.1894,\n",
       "                        0.1161,  0.1101],\n",
       "                      [ 0.0958, -0.1263,  0.0089,  0.2285,  0.0290,  0.2237,  0.3010,  0.2093,\n",
       "                       -0.1549,  0.1965],\n",
       "                      [-0.2946,  0.1556,  0.0484,  0.0556,  0.1687,  0.1476,  0.0572,  0.0531,\n",
       "                       -0.2877, -0.2995],\n",
       "                      [-0.0654,  0.2822, -0.2632,  0.0970,  0.2302, -0.0540, -0.0549,  0.0352,\n",
       "                        0.1165,  0.2228],\n",
       "                      [-0.1594, -0.2836, -0.1190,  0.0865,  0.2671,  0.0565, -0.0123,  0.2670,\n",
       "                       -0.2602, -0.0487],\n",
       "                      [ 0.1987,  0.2884,  0.0333,  0.0526,  0.0611,  0.1047, -0.0358,  0.1944,\n",
       "                       -0.2093, -0.0420],\n",
       "                      [ 0.3127, -0.0880, -0.2735, -0.1761,  0.1296, -0.2496,  0.1845,  0.1938,\n",
       "                       -0.3023, -0.2533],\n",
       "                      [ 0.0565, -0.1246, -0.0857, -0.2370,  0.2238,  0.1850, -0.0518,  0.1868,\n",
       "                        0.2337, -0.3114],\n",
       "                      [-0.1073, -0.0322, -0.1968, -0.2166,  0.1379,  0.1772,  0.0125,  0.1496,\n",
       "                       -0.1814,  0.2775],\n",
       "                      [ 0.0923,  0.0956,  0.1700,  0.2688,  0.0376, -0.1857,  0.0658, -0.0582,\n",
       "                        0.0036,  0.1748],\n",
       "                      [-0.1169, -0.1041, -0.0220, -0.2508,  0.1943, -0.1165,  0.0165,  0.1872,\n",
       "                        0.0637,  0.0660],\n",
       "                      [-0.2744,  0.2161,  0.2091,  0.1308, -0.1909,  0.1144, -0.0331,  0.0798,\n",
       "                        0.0513, -0.0359],\n",
       "                      [-0.2189, -0.1709, -0.0954,  0.2741, -0.0580,  0.1463, -0.1184,  0.1833,\n",
       "                        0.0850, -0.0079],\n",
       "                      [ 0.2523, -0.0307,  0.1010,  0.0189, -0.3096,  0.2352, -0.2236,  0.1166,\n",
       "                       -0.1269, -0.2617],\n",
       "                      [-0.0311, -0.0903,  0.2948, -0.0702,  0.1333, -0.0844, -0.2434,  0.0299,\n",
       "                        0.3075, -0.0084],\n",
       "                      [-0.1817, -0.2514,  0.1726, -0.0440,  0.1509,  0.0095,  0.1822,  0.2129,\n",
       "                       -0.1587, -0.2713],\n",
       "                      [ 0.3106,  0.1681,  0.1393,  0.2388, -0.1827, -0.0144,  0.2901,  0.3093,\n",
       "                       -0.2844, -0.1589],\n",
       "                      [-0.0573, -0.1246,  0.1351,  0.2800,  0.3153, -0.1463,  0.0124, -0.2293,\n",
       "                       -0.1170, -0.0278],\n",
       "                      [-0.0736,  0.0613, -0.2231,  0.0477, -0.1412, -0.1942,  0.0634, -0.2348,\n",
       "                        0.2110, -0.1207],\n",
       "                      [ 0.2938,  0.2540, -0.2262, -0.0829,  0.2818, -0.0791, -0.0246, -0.1385,\n",
       "                        0.1032,  0.2079],\n",
       "                      [-0.0985, -0.1420,  0.0029,  0.0088, -0.0949,  0.2259,  0.1040, -0.1435,\n",
       "                        0.2144,  0.1315],\n",
       "                      [-0.1160, -0.1231, -0.0552, -0.0681,  0.2320, -0.1310, -0.2204, -0.1579,\n",
       "                       -0.2150, -0.1954],\n",
       "                      [-0.1304,  0.2720, -0.3054, -0.0969,  0.1684,  0.0899,  0.0075,  0.3009,\n",
       "                        0.1401, -0.3138],\n",
       "                      [-0.3079, -0.0053,  0.0665,  0.0949, -0.0666,  0.2354, -0.0346, -0.0567,\n",
       "                        0.2506, -0.0513],\n",
       "                      [ 0.0538, -0.0931, -0.2388,  0.0036,  0.1283, -0.1810,  0.1543, -0.1074,\n",
       "                        0.1635,  0.2275],\n",
       "                      [ 0.0342, -0.2988,  0.1920, -0.1202, -0.0281, -0.0372, -0.2991, -0.1968,\n",
       "                       -0.1639,  0.0985],\n",
       "                      [ 0.0495,  0.2738,  0.1554,  0.1565,  0.2369, -0.0500, -0.0468, -0.1658,\n",
       "                        0.0661, -0.0593],\n",
       "                      [-0.1206, -0.0204,  0.2774, -0.1435,  0.0026, -0.0591,  0.1942, -0.0828,\n",
       "                        0.2093, -0.2686],\n",
       "                      [-0.0018, -0.1299,  0.0933,  0.0997, -0.1571, -0.2747, -0.1081, -0.0491,\n",
       "                       -0.0840,  0.2387],\n",
       "                      [-0.0389, -0.0320, -0.0485,  0.1108,  0.1676, -0.0177, -0.2004, -0.2505,\n",
       "                        0.1754,  0.0083],\n",
       "                      [ 0.2583,  0.1957,  0.2851,  0.2192,  0.1020,  0.1362,  0.0598,  0.2585,\n",
       "                       -0.1380,  0.2399],\n",
       "                      [ 0.0004, -0.3069, -0.3039, -0.1107,  0.2424,  0.0821,  0.2594,  0.2424,\n",
       "                       -0.2617, -0.1049],\n",
       "                      [-0.1027, -0.2108, -0.2866,  0.3085,  0.1397, -0.1945, -0.1825, -0.1752,\n",
       "                       -0.2694,  0.2148],\n",
       "                      [-0.0368,  0.0375,  0.0009, -0.1620, -0.3119, -0.1320,  0.2317,  0.2009,\n",
       "                        0.2682, -0.2287]])),\n",
       "             ('fc1.bias',\n",
       "              tensor([ 0.0213, -0.0675,  0.0032,  0.0644,  0.1487,  0.2104,  0.3098,  0.3033,\n",
       "                       0.2658,  0.3134,  0.1578, -0.1648,  0.1672,  0.3152,  0.0582,  0.1635,\n",
       "                       0.1436, -0.0539,  0.1232,  0.0457,  0.2750, -0.0536,  0.0772,  0.0683,\n",
       "                      -0.1939, -0.1217,  0.2978, -0.3077,  0.0626, -0.1319,  0.1610, -0.1553,\n",
       "                      -0.2775,  0.1525,  0.1795, -0.1868,  0.0744, -0.1262, -0.1063, -0.2828,\n",
       "                       0.0687,  0.1546,  0.2367, -0.1655,  0.2140, -0.3159, -0.1519, -0.3155,\n",
       "                      -0.2574,  0.0956])),\n",
       "             ('batch_norm.weight',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('batch_norm.bias',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0.])),\n",
       "             ('batch_norm.running_mean',\n",
       "              tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "                      0., 0.])),\n",
       "             ('batch_norm.running_var',\n",
       "              tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "                      1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])),\n",
       "             ('batch_norm.num_batches_tracked', tensor(0)),\n",
       "             ('fc2.weight',\n",
       "              tensor([[ 0.1139,  0.1340,  0.0683, -0.0900,  0.0115,  0.1048,  0.1389, -0.0597,\n",
       "                        0.0054, -0.0529,  0.0636,  0.0953, -0.1138, -0.0413, -0.1063,  0.0715,\n",
       "                        0.0923,  0.0485, -0.1082, -0.0828,  0.0805,  0.0190,  0.0457,  0.0836,\n",
       "                       -0.0781,  0.0177, -0.1222, -0.0277,  0.0243,  0.0933, -0.0513,  0.0847,\n",
       "                        0.1349, -0.1104, -0.0386,  0.0570, -0.1359,  0.0175,  0.0757, -0.0489,\n",
       "                        0.1231, -0.1047, -0.0935,  0.1147, -0.0855,  0.0334, -0.0401, -0.0949,\n",
       "                       -0.1400, -0.0905]])),\n",
       "             ('fc2.bias', tensor([-0.1138]))])"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the parameters\n",
    "new_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load our very first trained model parameters into the new one\n",
    "new_model.load_state_dict(torch.load('model_state_dict.pt'), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict({'fc1.weight': tensor([[-0.2281, -0.0470,  0.2462, -0.2436, -0.0946, -0.3025,  0.1570,  0.0669,\n",
      "          0.1495,  0.3131],\n",
      "        [ 0.1679, -0.3009,  0.3013,  0.1753,  0.0325, -0.1105, -0.2211, -0.2457,\n",
      "         -0.1122,  0.2106],\n",
      "        [-0.1523, -0.2123, -0.1113, -0.2280, -0.3060, -0.0035, -0.0418,  0.1317,\n",
      "         -0.2353, -0.0075],\n",
      "        [ 0.0966, -0.2417, -0.1267, -0.1652,  0.3085, -0.2116,  0.2211,  0.0841,\n",
      "         -0.1746, -0.1794],\n",
      "        [ 0.2723,  0.1501,  0.1777, -0.0608,  0.0209, -0.0566, -0.1187,  0.1044,\n",
      "          0.0284, -0.1925],\n",
      "        [ 0.2183, -0.0458, -0.0868,  0.2422,  0.2899,  0.0156,  0.2748,  0.1606,\n",
      "          0.1072,  0.2375],\n",
      "        [ 0.0581, -0.2766, -0.0214, -0.0603, -0.0570,  0.2813, -0.0172, -0.0519,\n",
      "          0.0022,  0.0754],\n",
      "        [ 0.0725,  0.0626,  0.1116,  0.0689, -0.0994, -0.1217, -0.2950, -0.0031,\n",
      "          0.2694,  0.2645],\n",
      "        [ 0.2746, -0.0389,  0.2735,  0.2441, -0.0763, -0.3111, -0.0373,  0.0636,\n",
      "          0.0860, -0.2441],\n",
      "        [ 0.0064, -0.2285, -0.1898, -0.1411, -0.1861, -0.0096,  0.1285,  0.2219,\n",
      "          0.1662, -0.0157],\n",
      "        [-0.0747, -0.2296,  0.0851, -0.1734, -0.2557,  0.3007, -0.1054,  0.2185,\n",
      "          0.2145, -0.0635],\n",
      "        [ 0.2970,  0.2983, -0.3039,  0.1446, -0.3140,  0.2920,  0.0456,  0.1560,\n",
      "         -0.3033, -0.1732],\n",
      "        [ 0.0781, -0.1572,  0.2380, -0.0472,  0.3029, -0.2903, -0.2456,  0.1636,\n",
      "         -0.2085, -0.1547],\n",
      "        [ 0.0918,  0.2721,  0.0193,  0.1818,  0.2572,  0.2404, -0.1114,  0.0277,\n",
      "          0.1991,  0.0253],\n",
      "        [-0.2932,  0.1860,  0.1188, -0.1237, -0.2008, -0.3025,  0.2590,  0.2382,\n",
      "         -0.0350, -0.1825],\n",
      "        [-0.0509,  0.1859, -0.0547, -0.2042, -0.1036,  0.1270,  0.1419, -0.2718,\n",
      "          0.1478, -0.0970],\n",
      "        [ 0.2410, -0.2508,  0.0375,  0.1061,  0.0459, -0.1370,  0.0766,  0.1059,\n",
      "          0.1208, -0.1475],\n",
      "        [ 0.2632, -0.3035, -0.2031,  0.2132, -0.0526, -0.0996, -0.2872, -0.1260,\n",
      "          0.1635, -0.1884],\n",
      "        [-0.0465, -0.1336,  0.3068,  0.0023,  0.3158,  0.1364, -0.2522, -0.2140,\n",
      "          0.2424, -0.0345],\n",
      "        [ 0.1399,  0.0352,  0.1466,  0.1052, -0.2308, -0.2266, -0.2135, -0.0760,\n",
      "         -0.1018,  0.2151],\n",
      "        [ 0.0167,  0.2342, -0.0384, -0.0987,  0.1121, -0.0773, -0.0085, -0.2762,\n",
      "          0.0664,  0.1935],\n",
      "        [ 0.2428, -0.0141,  0.0086, -0.2101,  0.0736,  0.0498, -0.0705,  0.0904,\n",
      "         -0.0031, -0.0033],\n",
      "        [-0.1832, -0.1448,  0.0237,  0.1667,  0.0632,  0.1030, -0.0926, -0.1052,\n",
      "         -0.3168, -0.1327],\n",
      "        [-0.2092,  0.1738, -0.2021, -0.0993,  0.1040, -0.0987, -0.0446,  0.1755,\n",
      "          0.1895,  0.3026],\n",
      "        [-0.2082,  0.0902,  0.0462, -0.1179, -0.1734, -0.1183,  0.0364,  0.1524,\n",
      "          0.1155, -0.2593],\n",
      "        [ 0.1292,  0.1977,  0.0829, -0.0960, -0.1147, -0.0841,  0.0449, -0.2928,\n",
      "         -0.2713,  0.2090],\n",
      "        [ 0.2754,  0.1924,  0.1055, -0.3104,  0.2113,  0.1109, -0.2185, -0.0219,\n",
      "         -0.0231,  0.0810],\n",
      "        [-0.0768,  0.1871,  0.2311,  0.2996, -0.2936,  0.0854,  0.1941,  0.1455,\n",
      "         -0.0419,  0.2741],\n",
      "        [-0.1068, -0.0813,  0.1485,  0.2701,  0.2544, -0.2536, -0.1561, -0.1533,\n",
      "          0.1819, -0.0440],\n",
      "        [ 0.0774, -0.2414,  0.2974, -0.1069, -0.2671,  0.0359,  0.0638, -0.0036,\n",
      "         -0.0826,  0.2264],\n",
      "        [-0.1613,  0.2212, -0.2752,  0.1842, -0.0741,  0.3236,  0.2209,  0.2583,\n",
      "          0.2525,  0.1071],\n",
      "        [ 0.0753, -0.0152,  0.0225, -0.0895, -0.2236,  0.2044, -0.0826,  0.0828,\n",
      "         -0.0624,  0.3090],\n",
      "        [-0.0958,  0.2934, -0.2066,  0.1436,  0.3043, -0.1578,  0.2110,  0.0016,\n",
      "         -0.0546, -0.1855],\n",
      "        [ 0.2702, -0.3058, -0.1253,  0.1570, -0.1868, -0.1908,  0.2847, -0.1948,\n",
      "          0.0488, -0.3051],\n",
      "        [-0.1609, -0.2001,  0.0810,  0.1916, -0.0171,  0.0852,  0.1612, -0.1006,\n",
      "         -0.2109,  0.1944],\n",
      "        [ 0.1038,  0.1784,  0.3120, -0.2651,  0.2889, -0.2975,  0.1589,  0.0753,\n",
      "          0.0799,  0.0479],\n",
      "        [-0.2891, -0.1799,  0.0698, -0.0096, -0.2280,  0.0819, -0.2850, -0.0004,\n",
      "          0.1457, -0.3094],\n",
      "        [ 0.2623,  0.1318,  0.2120, -0.0138,  0.2300, -0.0343,  0.0540,  0.1337,\n",
      "         -0.2589, -0.0809],\n",
      "        [ 0.1975,  0.1834,  0.0598, -0.2470, -0.1967,  0.3081, -0.1126,  0.2393,\n",
      "         -0.2513,  0.1264],\n",
      "        [ 0.0700,  0.3149,  0.2739,  0.1935,  0.2828, -0.0033,  0.0730,  0.0219,\n",
      "          0.1632, -0.0230],\n",
      "        [-0.1750,  0.2818, -0.1145,  0.0220, -0.0951,  0.0697,  0.0769,  0.1637,\n",
      "          0.1466, -0.2177],\n",
      "        [-0.0092, -0.2237,  0.2997,  0.2982, -0.1221, -0.2378, -0.2617,  0.2705,\n",
      "         -0.1801,  0.2027],\n",
      "        [ 0.2579,  0.0643,  0.1955,  0.2455, -0.2076, -0.0059,  0.1510,  0.2529,\n",
      "         -0.0356,  0.2311],\n",
      "        [-0.2007, -0.1020,  0.0481, -0.1144, -0.1572,  0.0342,  0.2170, -0.2722,\n",
      "         -0.2384, -0.2138],\n",
      "        [-0.0289, -0.0702, -0.0999,  0.2120,  0.2051, -0.2052, -0.2429, -0.0476,\n",
      "         -0.2127, -0.2123],\n",
      "        [ 0.1645, -0.0883,  0.1392, -0.2695,  0.0527, -0.1560,  0.1799,  0.0325,\n",
      "         -0.2489,  0.0007],\n",
      "        [-0.2690,  0.2777, -0.3070,  0.2841,  0.1150,  0.1437, -0.3067, -0.2156,\n",
      "         -0.2006, -0.2120],\n",
      "        [-0.0671,  0.2026, -0.2980, -0.0839,  0.2058, -0.0719,  0.0372,  0.2026,\n",
      "          0.1822, -0.3100],\n",
      "        [ 0.2541,  0.0179,  0.1774, -0.0238,  0.1415, -0.2737, -0.0448,  0.2231,\n",
      "         -0.2230,  0.0200],\n",
      "        [-0.1197,  0.2828, -0.1782,  0.0165, -0.2743,  0.1548, -0.0942, -0.2538,\n",
      "          0.0553,  0.0795]]), 'fc1.bias': tensor([-0.1211,  0.0148,  0.0378,  0.2120, -0.0413,  0.1078,  0.1217,  0.1700,\n",
      "         0.0570,  0.0143, -0.1204,  0.1273,  0.3029,  0.0832,  0.1496, -0.3070,\n",
      "        -0.0420, -0.1628,  0.2380,  0.2512, -0.0870, -0.0623, -0.0734, -0.1873,\n",
      "        -0.1965, -0.1507, -0.1256, -0.2728,  0.0560,  0.1510,  0.0036, -0.0980,\n",
      "        -0.0578,  0.1158,  0.2380, -0.2130,  0.1396,  0.1255, -0.0770, -0.1450,\n",
      "         0.1544,  0.1864, -0.0724,  0.1837, -0.1364, -0.2292, -0.1437,  0.2473,\n",
      "        -0.3051, -0.1837]), 'batch_norm.weight': tensor([0.9991, 0.9956, 0.9984, 0.9965, 0.9988, 0.9988, 0.9986, 0.9961, 0.9920,\n",
      "        0.9988, 0.9987, 0.9973, 0.9932, 0.9984, 0.9934, 0.9946, 1.0022, 0.9944,\n",
      "        0.9963, 0.9988, 0.9939, 0.9979, 0.9963, 0.9991, 0.9958, 0.9949, 0.9990,\n",
      "        0.9937, 0.9956, 0.9953, 0.9942, 0.9959, 0.9985, 0.9949, 0.9981, 0.9931,\n",
      "        0.9923, 0.9945, 0.9945, 0.9982, 0.9903, 0.9949, 0.9932, 0.9939, 0.9991,\n",
      "        0.9972, 0.9993, 0.9993, 0.9978, 0.9953]), 'batch_norm.bias': tensor([-0.0003, -0.0071, -0.0027, -0.0032,  0.0008, -0.0002, -0.0019,  0.0018,\n",
      "         0.0061, -0.0006, -0.0023, -0.0026,  0.0060, -0.0021,  0.0069, -0.0045,\n",
      "        -0.0022,  0.0033,  0.0027, -0.0012,  0.0065,  0.0021,  0.0049,  0.0010,\n",
      "        -0.0054, -0.0041, -0.0001, -0.0040,  0.0034,  0.0073,  0.0070, -0.0041,\n",
      "        -0.0016,  0.0063, -0.0009, -0.0058,  0.0044, -0.0040,  0.0080, -0.0023,\n",
      "        -0.0076,  0.0037, -0.0050, -0.0049, -0.0040,  0.0042, -0.0019, -0.0009,\n",
      "        -0.0022,  0.0057]), 'batch_norm.running_mean': tensor([0.1839, 0.2544, 0.2050, 0.3884, 0.1550, 0.3402, 0.2521, 0.3113, 0.2604,\n",
      "        0.2011, 0.1778, 0.4032, 0.4193, 0.2837, 0.2890, 0.0702, 0.1782, 0.2054,\n",
      "        0.3810, 0.3329, 0.1405, 0.1350, 0.1272, 0.1431, 0.0840, 0.1328, 0.1931,\n",
      "        0.1271, 0.2553, 0.2842, 0.2823, 0.1438, 0.2301, 0.3804, 0.3120, 0.1699,\n",
      "        0.2613, 0.2820, 0.2181, 0.1657, 0.2659, 0.3402, 0.1976, 0.2990, 0.1424,\n",
      "        0.0931, 0.2171, 0.4120, 0.0899, 0.1266]), 'batch_norm.running_var': tensor([0.1067, 0.1397, 0.0882, 0.1809, 0.0634, 0.1602, 0.0744, 0.1433, 0.1224,\n",
      "        0.0761, 0.0886, 0.2585, 0.2079, 0.1320, 0.1612, 0.0274, 0.0584, 0.0958,\n",
      "        0.1931, 0.1359, 0.0597, 0.0436, 0.0513, 0.0751, 0.0326, 0.0654, 0.1032,\n",
      "        0.0773, 0.1230, 0.1339, 0.1642, 0.0613, 0.1283, 0.2239, 0.1134, 0.1019,\n",
      "        0.1240, 0.1201, 0.1318, 0.0830, 0.1009, 0.1849, 0.1003, 0.1436, 0.0678,\n",
      "        0.0385, 0.1423, 0.1890, 0.0407, 0.0575]), 'batch_norm.num_batches_tracked': tensor(160), 'fc2.weight': tensor([[-2.0947e-02, -9.3771e-02, -3.7354e-02, -3.4113e-02, -1.5081e-02,\n",
      "         -3.1806e-02, -2.4540e-02,  9.3603e-03,  3.1444e-02,  9.0439e-05,\n",
      "         -6.7787e-03,  3.2002e-02,  7.4018e-02, -9.7669e-03,  6.5410e-02,\n",
      "         -3.5562e-02, -8.8005e-02,  7.2207e-03, -1.3096e-02,  8.7284e-03,\n",
      "          4.8867e-02, -5.7978e-03,  3.5426e-02, -4.2898e-02, -5.8037e-02,\n",
      "         -2.2508e-02, -1.6533e-02, -8.5152e-03, -1.9491e-02,  1.1168e-01,\n",
      "          9.1236e-02, -6.2677e-02,  3.2548e-02,  6.7610e-02,  8.5043e-03,\n",
      "         -6.2358e-02,  6.6322e-02, -4.9547e-02,  1.0249e-01,  3.1188e-02,\n",
      "         -5.1755e-02,  3.2463e-02, -4.3899e-02, -4.2352e-02, -7.1319e-02,\n",
      "          3.0172e-02, -4.0505e-02,  3.8722e-02, -2.0223e-02,  4.3903e-02]]), 'fc2.bias': tensor([-0.0302])})\n"
     ]
    }
   ],
   "source": [
    "# Show the new parameters\n",
    "print(new_model.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We would now take the parameters we just added into this model and train it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Loading Across Devices\n",
    "PyTorch supports multiple different devices such as CPU and GPUs.\n",
    "\n",
    "Its common practice to train on a GPU for speed but do inference on a CPU for cost for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model on CPU that was saved on GPU\n",
    "import torch\n",
    "\n",
    "model = torch.load('model_state_dict.pt', map_location='cpu', weights_only=True) # Using map_location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using device: mps — Apple Metal (MPS)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Detect the best available device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")   # NVIDIA GPU\n",
    "    device_name = torch.cuda.get_device_name(0)\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")    # Apple Silicon GPU\n",
    "    device_name = \"Apple Metal (MPS)\"\n",
    "else:\n",
    "    device = torch.device(\"cpu\")    # CPU fallback\n",
    "    device_name = \"CPU\"\n",
    "\n",
    "print(f\"✅ Using device: {device} — {device_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = torch.load('model_state_dict.pt', map_location=device, weights_only=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to above we must also put the model on the GPU:\n",
    "\n",
    "```py\n",
    "model.to('cuda')\n",
    "```\n",
    "\n",
    "As well as the inputs for inference.\n",
    "```py\n",
    "model.eval()\n",
    "outputs = model(sample_input.to('cuda'))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Auto-select the best available device (CUDA → MPS → CPU)\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'          # NVIDIA GPU\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = 'mps'           # Apple GPU (Metal)\n",
    "else:\n",
    "    device = 'cpu'           # CPU fallback\n",
    "\n",
    "print(device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
